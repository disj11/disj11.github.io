[{"content":"스프링 부트와 코틀린을 사용한 CSV 파일 다운로드 트러블슈팅 # 스프링 부트와 코틀린을 사용하여 CSV 파일 다운로드 기능을 구현하는 과정에서 발생한 여러 문제와 그 해결 방법에 대해 설명하겠습니다. 1. 한글 깨짐 문제 # 문제: CSV 파일을 다운로드하여 맥용 엑셀에서 열었을 때 한글이 깨지는 현상이 발생했습니다. 해결: API 서버에서 CSV 파일을 생성할 때 BOM(Byte Order Mark) 문자를 추가했습니다. BOM은 UTF-8로 인코딩된 파일의 시작 부분에 추가되는 특수한 문자 시퀀스로, 일부 애플리케이션(특히 Microsoft Excel)이 파일의 인코딩을 올바르게 인식하도록 돕습니다. 코드 예시: val bom = byteArrayOf(0xEF.toByte(), 0xBB.toByte(), 0xBF.toByte()) outputStream.write(bom) // CSV 내용 작성 코드 2. 프론트엔드에서의 다운로드 문제 # 문제: API 엔드포인트를 브라우저에 직접 입력하여 다운로드할 때는 문제가 없었지만, 프론트엔드에서 API를 호출하여 다운로드할 때 여전히 파일이 깨지는 현상이 발생했습니다. 해결: 프론트엔드에서 API를 호출할 때, 응답 타입을 \u0026lsquo;blob\u0026rsquo;으로 지정했습니다. \u0026lsquo;blob\u0026rsquo; 타입은 이진 데이터를 그대로 받아올 수 있게 해주어, 텍스트 인코딩 문제를 방지합니다. 코드 예시 (JavaScript): axios.get(\u0026#39;/api/download-csv\u0026#39;, { responseType: \u0026#39;blob\u0026#39; }) .then(response =\u0026gt; { // 파일 다운로드 로직 }) 3. CSV 형식 오류 # 문제: CSV 데이터에 큰따옴표(\u0026quot;)나 쉼표(,)가 포함된 경우 필드가 밀리는 문제가 발생했습니다. 해결: 필드에 쉼표, 큰따옴표, 개행 문자가 포함된 경우, 해당 필드 전체를 큰따옴표로 감쌌습니다. 필드 내의 큰따옴표는 두 번 연속 사용하여 이스케이프 처리했습니다. 이 해결 방법은 RFC 4180 표준을 참고하여 구현되었습니다. RFC 4180은 CSV 파일 형식에 대한 공식적인 명세를 제공하며, 특수 문자 처리에 대한 가이드라인을 포함하고 있습니다. RFC 4180에 따르면: 필드는 쉼표로 구분됩니다. 필드에 쉼표, 큰따옴표, 또는 줄바꿈이 포함된 경우 해당 필드를 큰따옴표로 묶어야 합니다. 필드 내에서 큰따옴표를 사용할 경우, 두 개의 연속된 큰따옴표로 이스케이프 처리해야 합니다. 코드 예시: fun escapeSpecialCharacters(field: String): String { return if (field.contains(\u0026#34;\\n\u0026#34;) || field.contains(\u0026#34;\\\u0026#34;\u0026#34;) || field.contains(\u0026#34;,\u0026#34;)) { \u0026#34;\\\u0026#34;${field.replace(\u0026#34;\\\u0026#34;\u0026#34;, \u0026#34;\\\u0026#34;\\\u0026#34;\u0026#34;)}\\\u0026#34;\u0026#34; } else { field } } // CSV 행 생성 시 val csvLine = listOf(field1, field2, field3) .map { escapeSpecialCharacters(it) } .joinToString(\u0026#34;,\u0026#34;) 이 방식을 적용함으로써 RFC 4180 표준을 준수하고, CSV 파일의 구조적 무결성을 유지할 수 있습니다. Apache Commons CSV 사용: 또 다른 해결 방법으로 Apache Commons CSV 라이브러리를 사용할 수 있습니다. 이 라이브러리는 CSV 파일 처리를 위한 강력하고 유연한 도구를 제공합니다. Apache Commons CSV를 사용하면 특수 문자 처리와 같은 복잡한 로직을 직접 구현할 필요 없이 CSV 파일을 쉽게 읽고 쓸 수 있습니다. 예를 들어: CSVPrinter csvPrinter = new CSVPrinter(writer, CSVFormat.DEFAULT); csvPrinter.printRecord(\u0026#34;John Doe\u0026#34;, \u0026#34;30\u0026#34;, \u0026#34;New York, NY\u0026#34;); 이 코드는 자동으로 필요한 이스케이프 처리를 수행하여 \u0026ldquo;New York, NY\u0026quot;와 같은 쉼표를 포함한 필드를 올바르게 처리합니다. Apache Commons CSV를 사용하면 CSV 형식 오류를 방지하고 표준을 준수하는 CSV 파일을 쉽게 생성할 수 있으며, 특수 문자 처리에 대한 걱정 없이 데이터를 안전하게 다룰 수 있습니다 결론 # CSV 파일 다운로드 기능 구현 시 다음 사항들을 고려해야 합니다: 인코딩 문제 해결을 위한 BOM 추가 프론트엔드에서의 올바른 응답 타입 설정 CSV 형식에 맞는 특수 문자 처리 이러한 방법들을 적용함으로써 안정적이고 정확한 CSV 파일 다운로드 기능을 구현할 수 있습니다.","date":"2025-01-18T13:47:47+09:00","href":"https://disj11.github.io/troubleshooting-for-csv-file-download/","objectID":"4edf7a376fc238ad7eca7c8b61ffb2fc_0","order":0,"tags":["TIL"],"title":"CSV 파일을 맥용 엑셀에서 볼 때 한글이 깨지는 문제"},{"content":"JetBrains Fleet에서 Kotest 사용하기 # JetBrains의 Fleet은 IntelliJ와 달리 Kotest 플러그인을 기본적으로 지원하지 않습니다. 따라서 Kotest 테스트를 실행하려면 약간의 설정이 필요합니다. 이 글에서는 Gradle을 사용하여 Kotest 테스트를 실행하는 방법과 Fleet에서 테스트 실행을 편리하게 설정하는 방법을 소개합니다. Gradle을 사용한 테스트 실행 # Gradle을 사용하면 아래 명령어를 통해 프로젝트의 모든 테스트를 실행할 수 있습니다: gradle test 특정 파일의 테스트만 실행하고 싶다면 --tests 옵션을 추가로 사용해야 합니다. 예를 들어, com.demo.TestFile 클래스의 테스트만 실행하려면 다음과 같이 입력합니다: gradle test --tests \u0026#34;com.demo.TestFile\u0026#34; CLI의 한계와 대안 # CLI를 사용하여 테스트를 실행하면 다음과 같은 불편함이 있을 수 있습니다: 어떤 테스트가 성공했는지 결과를 확인하기 어렵습니다. 특정 테스트만 실행하려면 매번 명령어를 입력해야 합니다. Fleet에서는 Run Configuration을 직접 생성하여 이러한 문제를 해결할 수 있습니다. Run Configuration 설정하기 # 아래와 같은 내용을 포함한 .fleet/run.conf 파일을 프로젝트에 추가합니다: { \u0026#34;configurations\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;gradle\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Run Gradle Test\u0026#34;, \u0026#34;tasks\u0026#34;: [ \u0026#34;test\u0026#34; ], \u0026#34;args\u0026#34;: [ \u0026#34;--tests\u0026#34;, \u0026#34;$FILE_NAME_NO_EXT$\u0026#34; ], \u0026#34;workingDir\u0026#34;: \u0026#34;$PROJECT_DIR$\u0026#34; } ] } FL-19204 참고 이 설정 파일은 Fleet의 Run 메뉴에 Run Gradle Test 항목을 추가합니다. 이를 통해 특정 테스트 파일의 테스트를 실행할 수 있으며, 결과를 IDE 내에서 쉽게 확인할 수 있습니다. 마무리 # JetBrains Fleet은 Kotest 플러그인을 기본 제공하지 않지만, 위와 같은 방법으로 Gradle과 Run Configuration을 활용하면 효율적으로 테스트를 실행하고 관리할 수 있습니다. 앞으로 Fleet에서 Kotest 플러그인을 지원하게 된다면 더욱 편리한 환경이 제공될 것으로 기대됩니다.","date":"2025-01-18T11:01:48+09:00","href":"https://disj11.github.io/jetbrains-fleet-kotest/","objectID":"3548eff28a13960cf8826a3347ca25fc_0","order":0,"tags":["TIL"],"title":"JetBrains Fleet에서 Kotest 사용하기"},{"content":"개요 # Kotlin과 Jackson을 사용할 때 primitive 타입의 필드가 누락된 경우, 예기치 않은 동작이 발생할 수 있습니다. 구체적으로: Kotlin의 non-null 타입으로 선언된 primitive 필드가 JSON에서 누락된 경우 Jackson이 이를 기본값(0, false 등)으로 처리하는 현상 String 과 같은 reference 타입이 누락된 경우는 예외가 발생하므로 혼란의 여지가 있음 이 글에서는 실제 사례와 테스트를 통해 이 문제를 재현하고, 다양한 해결 방안을 비교 분석하겠습니다. 문제 상황 # 현재 서비스에는 다음과 같은 데이터 클래스가 존재합니다: data class Video( val name: String, val durationMs: Long, ) 이를 REST API의 Request Body로 받고 있습니다: @RestController @RequestMapping(\u0026#34;/demo\u0026#34;) class DemoController { @PostMapping fun createVideo(@RequestBody request: Video): String { return request.toString() } } 예상되는 동작과 실제 동작 # 예상되는 동작 non-null로 선언된 필드가 누락된 경우 요청이 실패해야 함 특히 primitive 타입의 필드가 누락된 경우에도 동일하게 실패해야 함 실제 동작 String과 같은 reference 타입이 누락된 경우 400 Bad Request 발생 Long과 같은 primitive 타입이 누락된 경우 기본값(0)이 할당되어 요청이 성공 문제 검증 # API 테스트를 통한 검증 # 1. 정상적인 요청 curl -X POST \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;name\u0026#34;: \u0026#34;Sample Video\u0026#34;, \u0026#34;durationMs\u0026#34;: 3000 }\u0026#39; \\ http://localhost:8080/demo 결과: 200 OK 응답 2. name이 누락된 요청 curl -X POST \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;durationMs\u0026#34;: 3000 }\u0026#39; \\ http://localhost:8080/demo 결과: 400 Bad Request JSON parse error: Instantiation of [simple type, class com.example.demo.Video] value failed for JSON property name due to missing (therefore NULL) value for creator parameter name which is a non-nullable type 3. durationMs가 누락된 요청 curl -X POST \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;name\u0026#34;: \u0026#34;Sample Video\u0026#34; }\u0026#39; \\ http://localhost:8080/demo 결과: 예상과 달리 200 OK 응답 (durationMs가 0으로 설정됨) 단위 테스트를 통한 검증 # @WebMvcTest(DemoController::class) class DemoControllerTest : FunSpec() { @Autowired private lateinit var mockMvc: MockMvc init { extension(SpringExtension) test(\u0026#34;모든 필드가 포함된 경우 200 응답을 반환한다\u0026#34;) { val validJson = \u0026#34;\u0026#34;\u0026#34; { \u0026#34;name\u0026#34;: \u0026#34;Sample Video\u0026#34;, \u0026#34;durationMs\u0026#34;: 3000 } \u0026#34;\u0026#34;\u0026#34;.trimIndent() mockMvc.perform( MockMvcRequestBuilders.post(\u0026#34;/demo\u0026#34;) .contentType(MediaType.APPLICATION_JSON) .content(validJson) ).andExpect(MockMvcResultMatchers.status().isOk) } test(\u0026#34;name이 누락된 경우 400 응답을 반환한다\u0026#34;) { val invalidJson = \u0026#34;\u0026#34;\u0026#34; { \u0026#34;durationMs\u0026#34;: 3000 } \u0026#34;\u0026#34;\u0026#34;.trimIndent() mockMvc.perform( MockMvcRequestBuilders.post(\u0026#34;/demo\u0026#34;) .contentType(MediaType.APPLICATION_JSON) .content(invalidJson) ).andExpect(MockMvcResultMatchers.status().isBadRequest) } test(\u0026#34;durationMs가 누락된 경우 400 응답을 반환한다\u0026#34;) { val invalidJson = \u0026#34;\u0026#34;\u0026#34; { \u0026#34;name\u0026#34;: \u0026#34;Sample Video\u0026#34; } \u0026#34;\u0026#34;\u0026#34;.trimIndent() mockMvc.perform( MockMvcRequestBuilders.post(\u0026#34;/demo\u0026#34;) .contentType(MediaType.APPLICATION_JSON) .content(invalidJson) ).andExpect(MockMvcResultMatchers.status().isBadRequest) // 실패 } } } Jackson의 동작 방식 # Jackson은 missing 필드와 null 필드를 다르게 처리합니다. Missing 필드의 경우 primitive 타입에 대해 다음과 같은 기본값을 할당합니다: Long, Int: 0 Boolean: false Double, Float: 0.0 해결 방안 # 1. DeserializationFeature 설정 # 전역적으로 primitive 타입의 null 처리 방식을 변경하는 방법입니다. @Configuration class JacksonConfig { @Bean fun jackson2ObjectMapperBuilder(): Jackson2ObjectMapperBuilder { return Jackson2ObjectMapperBuilder() .featuresToEnable(DeserializationFeature.FAIL_ON_NULL_FOR_PRIMITIVES) } } 2. Nullable 타입 사용 # Kotlin의 null safety를 활용하여 명시적으로 null을 처리하는 방법입니다. data class Video( val name: String, val durationMs: Long? ) 3. JsonProperty 어노테이션 사용 # 필드별로 필수 여부를 지정하는 방법입니다. data class Video( val name: String, @JsonProperty(required = true) val durationMs: Long ) 각 해결방안의 장단점 # DeserializationFeature 설정 장점: 전역적으로 일관된 동작 보장 단점: 기존 코드에 영향을 줄 수 있음 Nullable 타입 사용 장점: Kotlin의 null safety 활용 가능 단점: null 처리 로직 추가 필요 JsonProperty 어노테이션 장점: 필드별로 세밀한 제어 가능 단점: 모든 필드에 개별적으로 설정 필요 권장되는 해결방안 # 새로운 프로젝트 시작 시 FAIL_ON_NULL_FOR_PRIMITIVES 설정 고려 기존 프로젝트의 경우 @JsonProperty(required = true) 사용 권장 Nullable이 허용되는 필드는 명시적으로 Long?과 같이 선언 결론 # Kotlin과 Jackson을 함께 사용할 때는 primitive 타입의 null 처리에 주의가 필요합니다. 실제 테스트를 통해 확인한 것처럼, 예상치 못한 동작이 발생할 수 있으므로 프로젝트의 요구사항과 상황에 맞는 적절한 해결방안을 선택하고, 일관된 규칙을 적용하는 것이 중요합니다.","date":"2025-01-11T16:40:31+09:00","href":"https://disj11.github.io/kotlin-jackson-primitive-type-null/","objectID":"d4c0ef0e494d065687d9f7676490b37c_0","order":0,"tags":["kotlin"],"title":"Kotlin과 Jackson 사용 시 주의할 점"},{"content":"문제 상황 # 서비스 운영 중 NullPointerException이 발생한 사례를 분석하였습니다. 특정 필드가 null이 아닌 상태로 로그에 출력되었음에도 불구하고, 이후 코드 실행 중 NullPointerException이 발생하는 이상 현상이 발견되었습니다. 문제는 동일한 코드를 다시 배포할 때마다 오류가 발생하거나 그렇지 않은 경우가 번갈아 나타나는 불안정한 상황이었습니다. 코드 및 로그 분석 # 다음은 문제의 코드와 로그입니다: fun doSomething() { val adGroup = findById(adGroupId) log.info { \u0026#34;광고그룹 정보: ${adGroup}\u0026#34; } // 광고그룹 정보: AdGroup(adProductId=3) 출력 requireNotNull(adGroup.adProductId) // NullPointerException 발생 } 위 로그에서는 adProductId가 3으로 출력되었으나, requireNotNull(adGroup.adProductId)에서 NullPointerException이 발생했습니다. 로그를 추가하여 확인한 결과 다음과 같은 현상이 나타났습니다: log.info { \u0026#34;adProductId: ${adGroup.adProductId}\u0026#34; } 이번에는 로그에 null이 찍혔습니다. 이 시점에서 문제의 원인이 엔티티 클래스에 있다고 생각하였고, 해당 엔티티 클래스에 아래와 같은 메서드가 정의되어 있음을 확인했습니다: @Entity class AdGroup( @Id ... @Column var adProductId: Long? = null ) { fun getAdProductId(): Long = requireNotNull(this.adProductId) } 로그를 다시 변경하여 테스트한 결과는 다음과 같습니다: println(\u0026#34;${adGroup.adProductId} || ${adGroup.getAdProductId()}\u0026#34;) 빌드 후 실행할 때마다 아래 두 가지 결과가 번갈아 출력되었습니다: null || 3 3 || 3 원인 분석 # 문제는 Kotlin과 Java 간의 언어적 차이와 Hibernate의 리플렉션 기반 구현 방식에서 비롯된 것으로 보입니다. 특히, 동일한 이름을 가진 여러 Getter 메서드가 존재할 경우 Hibernate Proxy가 예기치 못한 방식으로 동작할 수 있다는 점이 확인되었습니다. Kotlin과 Java의 차이 Java에서는 동일한 시그니처(메서드 이름과 매개변수 타입)를 가진 메서드를 허용하지 않지만, Kotlin에서는 반환 타입만 다른 메서드를 정의할 수 있습니다. 이는 Kotlin에서 허용되지만, Hibernate Proxy와 같은 리플렉션 기반 라이브러리에서는 혼란을 초래할 수 있습니다. Hibernate Proxy 동작 방식 Hibernate Proxy는 리플렉션 데이터를 기반으로 메서드를 호출합니다. 이 과정에서 리플렉션 데이터의 메서드 배열(publicMethods) 순서에 따라 호출되는 Getter 메서드가 달라질 수 있습니다. 디버깅 결과, 아래와 같은 동작이 확인되었습니다: 실패 시: public java.lang.Long org.xx.xx.getAdProductId() 호출 → null 성공 시: public long org.xx.xx.getAdProductId() 호출 → 3 팀원 분이 Hibernate 포럼에 문의한 결과, 다음과 같은 답변을 받았습니다: 이 문제는 이미 Kotlin 측에서 발생할 수 있는 예외적인 사례로 보이며, Hibernate 엔티티에서 여러 Getter 메서드를 가지는 속성을 사용하는 것은 권장하지 않습니다. 간단한 Java 애플리케이션에서 동일한 문제가 발생하는 사례를 주시면 도움을 드릴 수 있습니다. 하지만 JPA 명세에서 엔티티의 속성 접근자 메서드는 Java Beans 규약을 따라야 한다는 점을 유념하시기 바랍니다. Hibernate는 이러한 요구 사항에 대해 다소 관대하게 동작하려고 하지만, 여전히 이를 준수하는 것을 강력히 권장합니다. 원문: this already looks like an edge-case on the Kotlin side, and I would certainly discourage having properties with multiple getter methods on Hibernate entities in the first place. We can help you if you can demonstrate a problem that occurs with a simple Java application, but keep in mind that property accessor methods for entities in the JPA specification must follow the Java Beans conventions, and while Hibernate tries to be more lax about this requirement it’s still heavily encouraged. 해결 방안 # 커스텀 Getter 메서드 이름 변경 동일 이름의 Getter 메서드 충돌을 방지하기 위해 커스텀 메서드 이름을 변경합니다. fun fetchAdProductId(): Long = requireNotNull(this.adProcutId) @Transient 애노테이션 추가 @Transient 애노테이션을 추가합니다. @Transient fun getAdProductId(): Long = requireNotNull(this.adProcutId) Java Beans 규약 준수 JPA 엔티티 설계 시 Java Beans 규약을 준수하며, 동일 이름의 Getter 메서드를 피해야 합니다. 결론 # 이번 사례는 Kotlin과 Hibernate 간의 호환성 문제로 인해 발생한 것으로, 동일 이름의 Getter 메서드 사용이 원인이었습니다. 이를 통해 JPA 엔티티 설계 시 Java Beans 규약을 준수하는 것이 중요함을 다시 한번 확인할 수 있었습니다.","date":"2024-12-29T21:36:18+09:00","href":"https://disj11.github.io/inconsistent-hibernate-proxy-behavior-with-kotlin/","objectID":"f541e7de30cf68a66130cb17aebdf1fd_0","order":0,"tags":["TIL"],"title":"Hibernate Proxy 비정상 동작 사례 분석"},{"content":"Ktlint는 Kotlin 코드 스타일을 검사하고 포맷팅하는 도구로, 협업 시 코드의 일관성을 유지하는 데 유용합니다. Ktlint 1.0 버전부터 기본 설정이 ktlint_official 코드 스타일로 변경되었습니다. 만약 다른 스타일을 사용하고 싶다면, .editorconfig 파일의 ktlint_code_style 속성을 통해 이를 변경할 수 있습니다. 코드 스타일 변경하기 # .editorconfig 파일에서 아래와 같이 설정하여 원하는 코드 스타일을 지정할 수 있습니다: [*.{kt,kts}] ktlint_code_style = intellij_idea # 또는 android_studio, ktlint_official (기본값) 특정 규칙 비활성화하기 # 특정 규칙을 비활성화하려면 ktlint_ 접두사와 규칙 세트의 ID를 조합하여 설정하면 됩니다. 예를 들어, ktlint_official 코드 스타일을 사용하면서 standard 규칙 세트의 final-newline 규칙을 비활성화하려면 다음과 같이 설정합니다: [*.{kt,kts}] ktlint_code_style = ktlint_official ktlint_standard_final-newline = disabled IntelliJ에서 Ktlint 플러그인 활용하기 # IntelliJ IDEA를 사용하는 경우, Ktlint 플러그인을 설치하면 규칙 세트 ID와 규칙 이름을 쉽게 확인할 수 있습니다. 플러그인을 설치한 후, Settings \u0026gt; Tools \u0026gt; Ktlint에서 모드를 Manual로 변경하면 코드 스타일이 맞지 않는 경우 관련 정보를 확인할 수 있습니다: 규칙 비활성화 예시 # 위의 이미지는 IntelliJ에서 표시되는 특정 규칙의 예시입니다. 해당 규칙을 비활성화하려면 .editorconfig 파일에 다음과 같이 추가합니다: [*.{kt,kts}] ktlint_standard_no-multi-spaces = disabled 추가 참고 사항 # 전체 규칙 세트 비활성화: 특정 규칙 세트 전체를 비활성화하려면 아래와 같이 설정할 수 있습니다: [*.{kt,kts}] ktlint_standard = disabled # \u0026#39;standard\u0026#39; 규칙 세트 전체 비활성화 실험적 규칙 활성화: 실험적 규칙은 기본적으로 비활성화되어 있으며, 명시적으로 활성화해야 사용할 수 있습니다: [*.{kt,kts}] ktlint_experimental = enabled Pre-commit Hook 및 자동화 # Ktlint는 Git pre-commit hook과 통합하여 커밋 전에 자동으로 코드 스타일 검사를 실행할 수 있습니다. Gradle을 사용하는 경우 아래 명령어로 Git Hook을 추가할 수 있습니다: ./gradlew addKtlintCheckGitPreCommitHook 참고 자료 # 더 자세한 설정 옵션은 KtLint 공식 문서를 참조하세요.","date":"2024-02-22T20:46:04+09:00","href":"https://disj11.github.io/editorconfig/","objectID":"b7c956215f241b4a2f16a0223df89833_0","order":0,"tags":["kotlin"],"title":"Ktlint: 기본 설정 변경 및 커스터마이징 가이드"},{"content":"S3 Sink Connector에는 파일 로테이션을 설정할 수 있는 두 가지 주요 속성이 있습니다. 이번 포스트에서는 이 두 속성의 차이를 비교하고, 각각의 동작 방식을 자세히 살펴보겠습니다. rotate.interval.ms rotate.schedule.interval.ms 먼저 두 속성의 주요 차이를 표로 정리하였습니다. rotate.schedule.interval.ms rotate.interval.ms 기준 시간 시스템 시간 기준 timestamp.extractor를 통해 설정 (Kafka Record Time, Record Field, Wall Clock 등) 지속적인 데이터 스트림 필요 여부 필요하지 않음 필요함 Exactly-once 보장 여부 보장되지 않음 경우에 따라 보장됨 지속적인 데이터 유입 # rotate.schedule.interval.ms # 이 속성은 시스템 시간을 기준으로 일정 간격마다 파일을 플러시하고 S3에 업로드합니다. 지속적인 데이터 유입이 필요하지 않으며, 설정된 시간이 지나면 자동으로 파일이 커밋됩니다. 사용하기 위해서는 반드시 timezone 속성을 설정해야 합니다. 예를 들어, rotate.schedule.interval.ms 값을 3000ms로 설정한 경우를 살펴보겠습니다. 시간 (time) 오프셋 (Offset) 설명 (Description) 1706713200000 100 토픽 데이터 수신 1706713201000 101 토픽 데이터 수신 1706713202000 102 토픽 데이터 수신 1706713203000 n/a 파일 플러시 시작 위 예시에서, 데이터가 더 이상 들어오지 않더라도 지정된 시간(3000ms)이 지나면 파일이 플러시되고 업로드됩니다. rotate.interval.ms # 이 속성은 첫 번째 레코드의 타임스탬프를 기준으로 파일의 타임스탬프 범위를 계산합니다. 이후 레코드의 타임스탬프가 범위를 초과하면 파일을 플러시하고 S3에 업로드합니다. 지속적인 데이터 유입이 필요하며, 데이터가 중단되면 파일이 업로드되지 않고 열려 있는 상태로 남을 수 있습니다. 예를 들어, rotate.interval.ms 값을 3000ms로 설정한 경우를 살펴보겠습니다. 시간 (time) 오프셋 (Offset) 설명 (Description) 1706713200000 100 토픽 데이터 수신 1706713201000 101 토픽 데이터 수신 1706713202000 102 토픽 데이터 수신 1706713204000 103 타임스탬프 범위 초과로 파일 플러시 및 업로드 시작 1706713205000 104 토픽 데이터 수신 위 예시에서, 후속 레코드(Offset: 104)가 없으면 파일이 업로드되지 않을 가능성이 있으므로 주의가 필요합니다. Exactly-once Delivery # Exactly-once delivery를 보장하려면 기본적으로 rotate.interval.ms를 사용하는 것이 권장됩니다. 추가로 다양한 조건을 충족해야만 정확히 한 번만 데이터를 전달할 수 있습니다. Confluent 문서에서는 아래와 같은 이미지를 제공하여 Exactly-once delivery 조건을 설명합니다: 결론 # 두 속성은 서로 다른 목적과 환경에서 사용됩니다: rotate.schedule.interval.ms: 데이터 유입이 불규칙하거나 적은 경우 적합. rotate.interval.ms: 지속적인 데이터 유입이 보장되고 Exactly-once delivery가 필요한 경우 적합. 참고자료: Amazon S3 Sink Connector for Confluent Platform Amazon S3 Sink Connector for Confluent Cloud Partitioning Records into S3 Objects Confluent S3 Sink Connector EOS","date":"2024-01-10T16:42:15+09:00","href":"https://disj11.github.io/s3-sink-connector-scheduled-rotation/","objectID":"694dcce2f77b641a8389c0aa505c5f38_0","order":0,"tags":["kafka"],"title":"S3 Sink Connector Scheduled Rotation"},{"content":"Redis는 인기 있는 인메모리 데이터 구조 저장소로, 키 삭제를 위한 여러 방법을 제공합니다. 이 글에서는 DEL과 UNLINK 명령어의 차이점, 그리고 lazyfree-lazy-user-del 설정 옵션에 대해 살펴보며, Redis 키 삭제 전략을 최적화하는 데 도움을 드리고자 합니다. DEL 명령어 # Redis의 DEL 명령어는 데이터베이스에서 키를 제거하는 기본적인 작업입니다. 한 번의 작업으로 여러 키를 삭제할 수 있으며, 시간 복잡도는 O(N)입니다. 여기서 N은 제거할 키의 수입니다.1 시간 복잡도 분석 # 단순 키 타입(예: Strings): 키당 O(1) 복잡한 데이터 구조(예: Lists, Sets, Hashes): 키당 O(M), M은 데이터 구조 내 요소의 수 따라서, N개의 키를 삭제하는 총 시간 복잡도는 최악의 경우 O(N*M)로 표현될 수 있습니다. 성능 영향 # Redis는 주로 단일 스레드로 작동하기 때문에, 큰 키나 여러 복잡한 데이터 구조를 삭제하는 것은 서버를 차단하여 전체 성능에 영향을 줄 수 있습니다. 이러한 차단 동작은 특히 고처리량 환경에서 문제가 될 수 있습니다. UNLINK 명령어 # Redis 4.0에서 도입된 UNLINK 명령어는 키 삭제를 위한 비차단 대안을 제공합니다.23 주요 특징 # 일정한 시간 복잡도: 키 크기나 데이터 구조 복잡성에 관계없이 키당 O(1) 비동기 삭제: 키를 키스페이스에서 즉시 제거하고 실제 메모리 회수는 나중에 예약 비차단: 메모리가 해제되는 동안 Redis가 다른 명령을 계속 처리할 수 있음 내부 메커니즘 # UNLINK 명령어는 두 단계로 작동합니다: 키스페이스에서 키를 즉시 연결 해제합니다(O(1) 작업). 실제 메모리 회수를 백그라운드 스레드에서 비동기적으로 수행하도록 예약합니다. 작은 키에 대한 최적화 # 모든 UNLINK 작업이 비동기적으로 처리되는 것은 아닙니다. 매우 작은 키의 경우, 비동기 삭제를 예약하는 오버헤드가 즉시 삭제 비용을 초과할 수 있습니다. Redis는 내부적으로 삭제 비용을 계산하고, 비용이 특정 임계값을 초과하는 경우에만 비동기적으로 처리합니다. lazyfree.c 기반 간소화된 의사 코드4 if (deletion_cost \u0026gt; LAZYFREE_THRESHOLD) { schedule_async_deletion(key); } else { delete_immediately(key); } lazyfree-lazy-user-del 설정 # 기존 코드에서 DEL을 UNLINK로 변경하는 것이 실용적이지 않은 상황을 위해, Redis 6.0에서는 lazyfree-lazy-user-del 설정 옵션을 도입했습니다. 사용법 # Redis 설정에서 yes로 설정하면:5 lazyfree-lazy-user-del yes 이 옵션은 모든 DEL 명령어를 내부적으로 UNLINK로 처리하게 하여, 코드 변경 없이 비차단 삭제의 이점을 제공합니다. 결론 # DEL과 UNLINK의 차이를 이해하는 것은 특히 큰 키나 고처리량 시나리오를 다룰 때 Redis 성능을 최적화하는 데 중요합니다. DEL은 작은 키와 단순한 데이터 구조에 적합한 반면, UNLINK는 더 큰 데이터 세트와 복잡한 구조에 상당한 이점을 제공합니다. lazyfree-lazy-user-del 옵션은 광범위한 코드 수정 없이 이러한 이점을 활용할 수 있는 편리한 방법을 제공합니다. 사용 사례를 신중히 고려하고 적절한 삭제 전략을 구현함으로써 Redis 기반 애플리케이션의 성능과 응답성을 크게 향상시킬 수 있습니다. https://redis.io/docs/latest/commands/del/\u0026#160;\u0026#x21a9;\u0026#xfe0e; https://redis.io/docs/latest/commands/unlink/\u0026#160;\u0026#x21a9;\u0026#xfe0e; http://redisgate.kr/redis/command/unlink.php\u0026#160;\u0026#x21a9;\u0026#xfe0e; https://github.com/redis/redis/blob/unstable/src/lazyfree.c\u0026#160;\u0026#x21a9;\u0026#xfe0e; https://redis.io/docs/latest/operate/oss_and_stack/management/config-file/\u0026#160;\u0026#x21a9;\u0026#xfe0e;","date":"2023-10-20T20:57:17+09:00","href":"https://disj11.github.io/redis-del-command/","objectID":"574427588ebf40cd68081a2df6e03c44_0","order":0,"tags":["redis"],"title":"Redis DEL 과 UNLINK 의 차이"},{"content":"JIT Compiler란? # 자바 코드를 실행하기 위해서는 바이트 코드로 컴파일이 필요합니다. 바이트 코드는 JVM의 인터프리터를 통해 기계어로 해석된 후 실행됩니다. 하지만 인터프리터를 통해 해석되는 과정 때문에, 기계어로 직접 실행되는 언어에 비해 속도가 느릴 수 있습니다. 이러한 성능 차이를 해결하기 위해 JVM에서는 JIT(Just-In-Time) Compiler를 도입하였습니다. JIT Compiler의 작동 방식 # Oracle은 JDK 1.3부터 HotSpot이라는 가상 머신을 포함하고 있으며, 여기에는 두 가지 JIT 컴파일러가 포함되어 있습니다. C1(Client Compiler): 빠르게 실행되지만 최적화 수준이 낮은 코드를 생성합니다. C2(Server Compiler): 실행 시간이 더 오래 걸리지만, 더 최적화된 코드를 생성합니다. Tiered Compilation # JVM은 호출되는 메서드를 추적하며, 자주 호출되는 메서드를 C1 컴파일러를 사용해 컴파일합니다. 이후 호출 횟수가 증가하면 C2 컴파일러를 사용하여 다시 컴파일합니다. 이를 Tiered Compilation이라고 하며, 세부적인 컴파일 레벨은 다음과 같습니다: Level 0 – Interpreted Code Level 1 – Simple C1 Compiled Code Level 2 – Limited C1 Compiled Code Level 3 – Full C1 Compiled Code Level 4 – C2 Compiled Code 각 레벨에 대해 더 자세한 내용을 확인하고 싶다면 Compilation Levels을 참고합니다. 임계값 확인 및 설정 # JVM에서 메서드가 특정 레벨로 컴파일되기 위한 임계값은 다음 명령어로 확인할 수 있습니다: java -XX:+PrintFlagsFinal -version | grep Threshold | grep Tier 출력 예시: intx Tier3CompileThreshold = 2000 {product} {default} intx Tier3InvocationThreshold = 200 {product} {default} intx Tier3MinInvocationThreshold = 100 {product} {default} 여기서 Tier3 는 Level 3로 컴파일 되기 위한 임계값임을 나타내며 각 값의 의미는 아래와 같습니다: Tier3InvocationThreshold: 메서드 호출 횟수 임계값. Tier3BackEdgeThreshold: 반복문 등에서 이전 블록으로 돌아가는 분기 구문의 임계값. Tier3CompileThreshold: 메서드 호출 횟수와 백 엣지(back-edge) 임계값의 합. tiered compilation and CompileThreshold 에 메서드를 컴파일해야 하는지를 판단하기 위한 로직이 설명되어있습니다. 이를 간단히 코드로 표현해보면 다음과 같습니다: function shouldCompileMethod(invocationCount, backEdgeCount) { if (invocationCount \u0026gt; Tier3InvocationThreshold) { return true; } if (invocationCount \u0026gt; Tier3MinInvocationThreshold \u0026amp;\u0026amp; invocationCount + backEdgeCount \u0026gt; Tier3CompileThreshold ) { return true; } return false; } 이를 바탕으로 메서드가 호출될 때마다 20개의 back-edge가 생성된다고 가정하면, 이 메서드는 약 100번 호출될 때 Level 3로 컴파일될 수 있음을 파악할 수 있습니다.: $$ 100 + (20 \\times 100) \u003e 2000 $$100 + (20 * 100) \u0026gt; 2000 --- ---------- ---- 1 2 3 1: 메서드 호출 횟수 2: back-edge 3: Tier3CompileThreshold 최적화 확인 방법 # JIT Compiler의 최적화를 확인하려면 아래 VM 옵션을 추가하여 로그를 생성할 수 있습니다: -XX:+UnlockDiagnosticVMOptions -XX:+LogCompilation 옵션을 추가하고 아래의 샘플 코드를 통해 실제로 최적화가 일어나는지 확인해보았습니다: fun main() { val arr = intArrayOf(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) (1..3000).forEach { i -\u0026gt; findMax(arr) if (i % 100 == 0) { // 너무 빨리 종료되면 c2 컴파일이 안될 수도 있기 때문에 sleep 을 넣어줌 Thread.sleep(100) } } } fun findMax(arr: IntArray): Int { var max = arr[0] for (i in 1 until arr.size) { if (max \u0026lt; arr[i]) { max = arr[i] } } return max } 프로그램 실행 후 생성된 로그 파일(hotspot_pid\u0026lt;pid\u0026gt;.log)에서 다음과 같이 level 3 컴파일을 위해 c1 queue 에 메서드가 적재된 것을 확인할 수 있었습니다: \u0026lt;task_queued compile_id=\u0026#39;205\u0026#39; method=\u0026#39;com.kotlin.JitTestKt findMax ([I)I\u0026#39; bytes=\u0026#39;39\u0026#39; count=\u0026#39;228\u0026#39; backedge_count=\u0026#39;2048\u0026#39; iicount=\u0026#39;228\u0026#39; level=\u0026#39;3\u0026#39; stamp=\u0026#39;0.438\u0026#39; comment=\u0026#39;tiered\u0026#39; hot_count=\u0026#39;228\u0026#39;/\u0026gt; count 와 backedge_count 를 통해 메서드가 몇 번 호출되었는지와 백엣지 수를 확인할 수 있으며, 조금 더 아래 로그를 살펴보니 다음과 같이 level 3 로 코드 최적화가 된 것을 확인할 수 있었습니다: \u0026lt;nmethod compile_id=\u0026#39;205\u0026#39; compiler=\u0026#39;c1\u0026#39; level=\u0026#39;3\u0026#39; entry=\u0026#39;0x00000208cb1883a0\u0026#39; size=\u0026#39;2560\u0026#39; address=\u0026#39;0x00000208cb188190\u0026#39; relocation_offset=\u0026#39;344\u0026#39; insts_offset=\u0026#39;528\u0026#39; stub_offset=\u0026#39;1872\u0026#39; scopes_data_offset=\u0026#39;2040\u0026#39; scopes_pcs_offset=\u0026#39;2216\u0026#39; dependencies_offset=\u0026#39;2520\u0026#39; nul_chk_table_offset=\u0026#39;2528\u0026#39; oops_offset=\u0026#39;1992\u0026#39; metadata_offset=\u0026#39;2008\u0026#39; method=\u0026#39;com.kotlin.JitTestKt findMax ([I)I\u0026#39; bytes=\u0026#39;39\u0026#39; count=\u0026#39;228\u0026#39; backedge_count=\u0026#39;2048\u0026#39; iicount=\u0026#39;228\u0026#39; stamp=\u0026#39;0.438\u0026#39;/\u0026gt; JIT Compiler 활성화/비활성화 성능 비교 # 아래 코드는 JIT Compiler 활성화 여부에 따라 실행 시간을 비교한 결과입니다. -Xint 옵션으로 Jit Compiler 사용을 중지하고 테스트 해 볼 수 있습니다: fun main() { val arr = intArrayOf(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) val startTimeMills = System.currentTimeMillis() (1..1_0000_0000).forEach { findMax(arr) } println(\u0026#34;실행시간: ${System.currentTimeMillis() - startTimeMills}ms\u0026#34;) } fun findMax(arr: IntArray): Int { var max = arr[0] for (i in arr.indices) { if (max \u0026lt; arr[i]) max = arr[i] } return max } 결과 # JIT Compiler 활성화 실행시간: 약 875~1595ms JIT Compiler 비활성화 (-Xint 옵션 사용) 실행시간: 약 24,760~32,401ms 이처럼 JIT Compiler는 프로그램의 성능을 크게 향상시킵니다. 주의사항: Spring Boot 애플리케이션을 IntelliJ에서 실행할 경우 -XX:TieredStopAtLevel=1 옵션이 자동 추가되어 Level 1까지만 컴파일됩니다(IDEA-297872). 정확한 테스트를 위해서는 Run/Degub Configurations -\u0026gt; Modify options -\u0026gt; Disabled launch Optimization 옵션을 체크하여 실행해야 합니다. 참고 자료: https://www.baeldung.com/jvm-tiered-compilation https://www.baeldung.com/graal-java-jit-compiler https://www.lmax.com/blog/staff-blogs/2016/03/05/observing-jvm-warm-effects/ https://www.oreilly.com/library/view/java-performance-the/9781449363512/ch04.html https://hg.openjdk.org/jdk8/jdk8/hotspot/file/104743074675/src/share/vm/runtime/advancedThresholdPolicy.hpp https://www.youtube.com/watch?v=CQi3SS2YspY\u0026list=PLyGtIjZ_uWKNT5-ob1TL26hH0KVfoJQzw","date":"2023-05-10T23:56:05+09:00","href":"https://disj11.github.io/java-jit-compiler/","objectID":"4658aac5581ac910f876ff199ba0868c_0","order":0,"tags":["development"],"title":"JIT Compiler"},{"content":"Amazon SQS(Simple Queue Service)는 두 가지 유형의 큐를 제공하고 있습니다. 각각의 특성과 사용 사례에 대해 자세히 알아보겠습니다. Standard Queue # Standard Queue는 무제한에 가까운 처리량(throughput)을 제공하는 것이 특징입니다. 다음과 같은 특성을 가지고 있습니다: 메시지 전달 특성 At-least-once delivery: 메시지가 최소 한 번 이상 전달됨을 보장합니다. 때로는 동일한 메시지가 여러 번 전달될 수 있습니다. Best-Effort Ordering: 메시지 순서가 보장되지 않을 수 있으며, 상황에 따라 전송된 순서와 다르게 수신될 수 있습니다. 주요 고려사항 애플리케이션은 반드시 멱등성(idempotent)을 보장해야 합니다. 즉, 동일한 메시지가 여러 번 처리되더라도 시스템에 영향을 주지 않도록 설계되어야 합니다. 높은 처리량이 요구되는 시스템에 적합합니다. FIFO Queue # FIFO(First-In-First-Out) Queue는 메시지의 순서와 정확성을 보장하는 것이 특징입니다. 메시지 전달 특성 Exactly-Once Processing: 메시지가 정확히 한 번만 처리됨을 보장합니다. First-In-First-Out Delivery: 메시지가 전송된 순서대로 정확하게 수신됨을 보장합니다. 주요 사용 사례 이벤트의 순서가 중요한 비즈니스 프로세스에 적합합니다. 정확한 순서 처리가 필요한 금융 거래나 주문 처리 시스템에 활용됩니다. 참고 사이트: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-queue-types.html","date":"2023-05-06T12:55:31+09:00","href":"https://disj11.github.io/sqs-queue-types/","objectID":"109cba42c1c691bccb86eaa42fac11ff_0","order":0,"tags":["TIL"],"title":"AWS SQS의 Queue 유형별 특성과 활용 방안"},{"content":"메인 시퀀스로부터의 거리(Distance from the Main Sequence)는 아키텍처 구조를 평가하는 여러 메트릭 중 하나입니다. 이는 불안정도(instability) 와 추상도(abstractness) 를 이용해 계산되므로, 두 개념을 먼저 이해하는 것이 중요합니다. 추상도 (Abstractness) # 추상도는 모듈 내의 추상 아티팩트(abstract artifact) 와 구상 아티팩트(concrete artifact) 의 비율을 나타내며, 구현 대비 추상화 정도를 측정합니다. 추상 아티팩트에는 추상 클래스나 인터페이스가 포함되며, 구상 아티팩트는 일반 클래스(구현체)를 의미합니다. 추상도를 계산하는 공식은 다음과 같습니다: $$ A = \\frac{\\sum m^a}{\\sum m^c} $$여기서: \\(\\sum m^a\\): 모듈 내 추상 요소(인터페이스 또는 추상 클래스)의 총합 \\(\\sum m^c\\): 모듈 내 구상 요소(일반 클래스)의 총합 추상도의 값은 0에서 1 사이에 위치하며, 1에 가까울수록 해당 모듈이 완전히 추상적임을 의미합니다. 예를 들어, 5,000 라인의 코드를 모두 main() 메서드에 구현한 애플리케이션은 구상 요소만 포함하고 있으므로 추상도는 0에 가깝습니다. 불안정도 (Instability) # 불안정도는 코드의 변동 가능성(volatility)을 나타내며, 원심 커플링(efferent coupling)과 구심 커플링(afferent coupling)의 비율로 계산됩니다. 공식은 다음과 같습니다: $$ I = \\frac{C^e}{C^a + C^e} $$여기서: \\(C^e\\): 원심 커플링(다른 코드 아티팩트로 유출되는 의존성의 수) \\(C^a\\): 구심 커플링(다른 코드 아티팩트로부터 유입되는 의존성의 수) 불안정도의 값 역시 0에서 1 사이에 위치하며, 값이 1에 가까울수록 해당 모듈이 외부 의존성이 많아 변동 가능성이 높음을 의미합니다. 예를 들어, 여러 클래스를 호출하는 클래스는 호출된 메서드 중 하나라도 변경될 경우 해당 클래스가 영향을 받을 가능성이 높아 불안정도가 높다고 볼 수 있습니다. 메인 시퀀스로부터의 거리 (Distance from the Main Sequence) # 메인 시퀀스로부터의 거리는 불안정도와 추상도를 결합하여 계산되며, 다음 공식을 사용합니다: $$ D = |A + I - 1| $$여기서: \\(A\\): 추상도 \\(I\\): 불안정도 이 값은 항상 0에서 1 사이에 위치하며, 값이 작을수록 해당 모듈이 이상적인 균형 상태에 가까움을 나타냅니다. 이를 시각적으로 표현하면 다음과 같은 그래프가 됩니다: x축: 불안정도 (\\(I\\)) y축: 추상도 (\\(A\\)) 메인 시퀀스: \\(A + I = 1\\)을 만족하는 선 메인 시퀀스에 가까운 모듈일수록 추상화와 안정성 간의 균형이 잘 이루어진 상태입니다. 쓸모없는 구역과 고통스러운 구역 # 그래프에서 메인 시퀀스로부터 멀어진 영역은 두 가지로 나뉩니다: 쓸모없는 구역 (Zone of Uselessness): 그래프의 오른쪽 위에 위치하며, 지나치게 높은 추상도로 인해 실제 구현에서 사용하기 어려운 코드를 나타냅니다. 고통스러운 구역 (Zone of Pain): 그래프의 왼쪽 아래에 위치하며, 거의 추상화되지 않은 상태로 인해 취약하고 유지보수가 어려운 코드를 나타냅니다. 따라서 메인 시퀀스와의 거리를 줄이는 것이 코드 품질과 유지보수성을 높이는 데 중요한 목표가 됩니다.","date":"2023-03-12T14:09:09+09:00","href":"https://disj11.github.io/distance-from-the-main-sequence/","objectID":"0b2a90ca1841f6eeb373c950ccca4592_0","order":0,"tags":["software engineering"],"title":"메인 시퀀스로부터의 거리: 추상도와 불안정도를 활용한 아키텍처 평가"},{"content":"응집 (Cohesion) # 응집은 소프트웨어 모듈 내 구성 요소들이 얼마나 밀접하게 연관되어 있는지를 나타내는 개념으로, 높은 응집도는 모듈의 독립성과 유지보수성을 높이는 데 기여합니다. 응집은 여러 유형으로 나뉘며, 각 유형은 모듈 내 구성 요소들의 관계와 협력 방식을 기준으로 정의됩니다. 아래에서는 각 응집 유형을 예시와 함께 설명합니다. 1. 기능적 응집 (Functional Cohesion) # 모듈 내 모든 구성 요소가 단일한 목적을 위해 협력하며, 특정 작업을 완벽히 수행하기 위해 필요한 모든 기능이 포함된 경우입니다. 이는 가장 이상적인 응집 형태로 간주됩니다. 예시: 계산기 프로그램에서 사각형의 넓이와 둘레를 계산하는 모듈은 입력값(가로, 세로)을 받아 넓이와 둘레를 계산한 후 결과를 반환합니다. 이 모듈은 단일 작업(사각형 계산)에 집중되어 있어 기능적 응집을 가집니다. 2. 순차적 응집 (Sequential Cohesion) # 모듈 내 구성 요소들이 순차적으로 실행되며, 하나의 구성 요소 출력이 다음 구성 요소의 입력으로 사용되는 경우입니다. 예시: 데이터 처리 파이프라인을 생각해볼 수 있습니다. 예를 들어, 파일에서 데이터를 읽고 → 데이터를 정제하고 → 정제된 데이터를 데이터베이스에 저장하는 작업이 순서대로 이루어지는 모듈은 순차적 응집을 가집니다. 3. 소통적 응집 (Communicational Cohesion) # 모듈 내 구성 요소들이 공통 데이터를 사용하거나 동일한 데이터를 기반으로 작업하는 경우입니다. 예시: 고객의 장바구니 데이터를 처리하는 모듈에서는 장바구니 데이터를 기반으로 할인 계산, 배송비 계산, 세금 계산 등의 작업이 이루어질 수 있습니다. 이처럼 구성 요소들은 서로 다른 작업을 수행하지만, 동일한 데이터를 공유하며 협력합니다. 4. 절차적 응집 (Procedural Cohesion) # 구성 요소들이 특정 절차나 순서를 따라 실행되도록 그룹화된 경우입니다. 이는 순차적 응집과 유사하지만, 반드시 동일한 데이터를 다루지 않을 수도 있습니다. 예시: 사용자 인증 모듈에서 사용자의 자격 증명을 확인하고 → 액세스 토큰을 생성하며 → 사용자 활동 로그를 업데이트하는 과정은 절차적 응집의 예입니다. 5. 일시적 응집 (Temporal Cohesion) # 모듈 내 구성 요소들이 특정 시간이나 이벤트에 따라 함께 실행되는 경우입니다. 예시: 시스템 초기화 시, 로그 파일 생성, 설정 파일 로드, 캐시 초기화 등 서로 연관성이 없어 보이는 작업들이 동시에 실행된다면 이는 일시적 응집에 해당합니다. 6. 논리적 응집 (Logical Cohesion) # 구성 요소들이 기능적으로 연관되기보다는 논리적인 범주에 따라 그룹화된 경우입니다. 예시: 자바의 StringUtils 클래스처럼 문자열 관련 다양한 정적 메서드(문자열 대소문자 변환, 문자열 자르기 등)가 포함된 경우입니다. 이들은 논리적으로는 관련이 있지만, 기능적으로는 독립적입니다. 7. 동시적 응집 (Coincidental Cohesion) # 모듈 내 구성 요소들이 단순히 같은 소스 파일에 포함되어 있을 뿐, 서로 아무런 연관성이 없는 경우입니다. 이는 가장 낮은 수준의 응집도로 간주됩니다. 예시: 한 모듈에 문자열 출력 함수와 리스트 정렬 함수가 함께 포함되어 있다면 이는 동시적 응집의 예로 볼 수 있습니다. 이러한 설계는 유지보수성과 재사용성을 저하시킵니다. LCOM (Lack of Cohesion in Methods) # 코드의 응집도를 정량적으로 평가하기 위해 LCOM(Lack of Cohesion in Methods) 메트릭을 사용할 수 있습니다. 이는 클래스 내 메서드들이 공유 필드를 얼마나 활용하는지 분석하여 응집도를 측정합니다. LCOM 활용 예: 클래스 X, Y, Z 비교 # 이미지에 나타난 클래스 X, Y, Z는 LCOM(Lack of Cohesion in Methods, 메서드 간 응집 결여도)을 계산하고 이해하는 데 유용한 사례를 제공합니다. 각 클래스의 구조를 분석하여 응집도를 평가해보겠습니다. 1. 클래스 X # 구성: 필드 A, B, C (육각형) 메서드 m1(), m2(), m3() (사각형) 각 메서드는 여러 필드를 공유하며 서로 연결되어 있습니다. 분석: 클래스 X는 모든 필드(A, B, C)가 여러 메서드(m1(), m2(), m3())에 의해 공유되고 사용됩니다. 이는 메서드와 필드가 서로 밀접하게 연관되어 있음을 나타냅니다. LCOM 평가: LCOM 점수가 낮습니다(즉, 응집도가 높음). 이는 클래스가 단일한 목적을 가지고 있으며, 메서드들이 협력하여 작업을 수행한다는 것을 보여줍니다. 2. 클래스 Y # 구성: 필드 A, B, C 메서드 m1(), m2(), m3() 각 필드는 오직 하나의 메서드에서만 사용됩니다. 분석: 클래스 Y에서는 각 메서드가 특정 필드만 사용하며 다른 필드나 메서드와 상호작용하지 않습니다. 이는 메서드들이 독립적으로 작동한다는 것을 의미합니다. LCOM 평가: LCOM 점수가 매우 높습니다(즉, 응집도가 낮음). 이 경우 클래스 Y는 단일 클래스로 유지할 필요가 없으며, 각 필드와 관련된 메서드를 별도의 클래스로 분리하는 것이 더 바람직합니다. 3. 클래스 Z # 구성: 필드 A, B, C 메서드 m1(), m2(), m3() 일부 메서드는 여러 필드를 공유하며 상호작용하지만, 특정 필드는 독립적으로 사용됩니다. 분석: 클래스 Z는 일부 메서드와 필드가 서로 연관되어 있지만, 다른 구성 요소들은 독립적으로 작동합니다. 이는 클래스 내에서 응집도가 부분적으로 유지되고 있음을 나타냅니다. LCOM 평가: LCOM 점수는 중간 수준입니다. 독립적인 구성 요소(예: C와 관련된 m3())를 별도의 클래스로 분리하면 응집도를 향상시킬 수 있습니다. 결론 # 클래스 X는 높은 응집도를 가지며 잘 설계된 구조입니다. 클래스 Y는 낮은 응집도를 가지며, 리팩토링을 통해 각 필드와 관련된 메서드를 별도의 클래스로 분리하는 것이 적합합니다. 클래스 Z는 부분적으로 응집되어 있으며, 독립적인 구성 요소를 분리하여 개선할 여지가 있습니다. 이처럼 LCOM 분석은 클래스의 구조적 결함을 식별하고 설계를 개선하는 데 유용한 도구로 활용될 수 있습니다. 결론 # 응집도는 소프트웨어 설계 품질을 평가하는 중요한 척도이며, 높은 응집도를 유지하는 것이 바람직합니다. 각 유형의 응집을 이해하고 이를 설계에 적용함으로써 더 나은 모듈화와 유지보수성을 달성할 수 있습니다.","date":"2023-03-12T11:41:11+09:00","href":"https://disj11.github.io/cohesion-in-software-engineering/","objectID":"a35cbd56dc52fb38d6b50c76480a89b6_0","order":0,"tags":["software engineering"],"title":"소프트웨어 응집도와 LCOM: 이해와 활용"},{"content":"확장성 # 확장성은 시스템이 증가하는 부하에 효과적으로 대처할 수 있는 능력을 설명하는 데 사용되는 용어입니다. 하지만 \u0026ldquo;X는 확장 가능하다\u0026rdquo; 또는 \u0026ldquo;Y는 확장성이 없다\u0026quot;라는 표현은 구체적인 의미를 전달하기 어렵습니다. 확장성을 논의한다는 것은 \u0026ldquo;시스템이 커진다면 이에 대처하기 위한 선택은 무엇인가?\u0026rdquo; 그리고 \u0026ldquo;추가 부하를 처리하기 위해 자원을 어떻게 투입할 것인가?\u0026rdquo; 와 같은 구체적인 질문을 고려한다는 뜻입니다. 확장성을 평가하려면 시스템이 증가하는 부하에 어떻게 반응하는지, 이를 해결하기 위해 어떤 전략을 사용할 수 있는지를 명확히 이해해야 합니다. 특히, 수직적 확장(scale-up)과 수평적 확장(scale-out)의 차이를 이해하는 것이 중요합니다. 수직적 확장(scale-up): 기존 시스템의 성능을 높이기 위해 더 강력한 하드웨어를 추가하거나 업그레이드하는 방식입니다. 예를 들어, 더 빠른 CPU나 더 많은 메모리를 추가하는 것이 이에 해당합니다. 수평적 확장(scale-out): 여러 대의 시스템을 추가하여 부하를 분산시키는 방식입니다. 예를 들어, 웹 서버를 여러 대로 늘리고 로드 밸런서를 통해 트래픽을 분산시키는 방법이 있습니다. 각 방법에는 장단점이 있으며, 상황에 따라 적합한 전략을 선택해야 합니다. 부하 기술하기 # 확장성을 논의하려면 현재 시스템의 부하를 간결하고 명확하게 기술해야 합니다. 이를 통해 \u0026ldquo;부하가 두 배로 증가하면 어떤 일이 발생할까?\u0026rdquo; 와 같은 시나리오를 분석할 수 있습니다. 부하는 부하 매개변수(load parameter) 라는 몇 가지 숫자로 표현됩니다. 시스템마다 적합한 부하 매개변수는 다르지만, 일반적으로 다음과 같은 항목들이 포함됩니다: 웹 서버의 초당 요청 수 데이터베이스의 읽기 대 쓰기 비율 동시 활성 사용자(active user) 수 캐시 적중률(cache hit rate) 이러한 매개변수는 평균적인 상황을 나타낼 수도 있지만, 때로는 극단적인 경우(예: 피크 타임, 병목 현상)가 시스템 성능에 더 큰 영향을 미칠 수 있습니다. 따라서 평균뿐만 아니라 극단적인 상황도 함께 고려해야 합니다. 예를 들어, 대규모 전자상거래 플랫폼에서는 초당 요청 수와 동시 활성 사용자가 중요한 부하 매개변수가 될 수 있습니다. 반면, 스트리밍 서비스에서는 캐시 적중률과 네트워크 대역폭 사용량이 주요 지표가 될 것입니다. 성능 기술하기 # 부하를 정의한 후에는 부하가 증가했을 때 시스템 성능이 어떻게 변화하는지 조사할 필요가 있습니다. 이를 위해 다음과 같은 질문을 던질 수 있습니다: 부하 매개변수를 증가시키고 자원(CPU, 메모리, 네트워크 대역폭)은 그대로 유지하면 성능에 어떤 영향을 미칠까요? 부하 매개변수를 증가시키면서 성능을 유지하려면 자원을 얼마나 추가해야 할까요? 이 질문들에 답하려면 시스템 성능을 측정하고 평가할 수 있는 지표가 필요합니다. 주요 성능 지표 # 일괄 처리 시스템(batch processing system): 주로 처리량(throughput) 에 관심을 둡니다. 예: 초당 처리 가능한 레코드 수 또는 일정 크기의 데이터 집합을 처리하는 데 걸리는 시간. 온라인 시스템(online system): 주로 응답 시간(response time) 에 관심을 둡니다. 응답 시간은 클라이언트가 요청을 보내고 응답을 받기까지 걸리는 시간으로 정의됩니다. 응답 시간은 고정된 값이 아니라 요청마다 변동될 수 있으므로, 단일 숫자가 아닌 분포(distribution) 로 이해해야 합니다. 응답 시간 분석 # 평균 응답 시간은 전체적인 경향을 파악하기에는 유용하지만, 실제 사용자 경험을 정확히 반영하지 못할 수 있습니다. 이는 평균값이 극단적인 값(특이 값, outlier)에 의해 왜곡될 가능성이 있기 때문입니다. 따라서 평균 대신 백분위(percentile) 를 사용하는 것이 더 적합합니다. 중앙값(median): 응답 시간을 빠른 순서대로 정렬했을 때 중간값으로, 50%의 요청이 이 시간 이하로 처리됨을 의미합니다. 상위 백분위: 95분위(p95), 99분위(p99), 99.9분위(p999) 등이 자주 사용됩니다. 예: 95분위 응답 시간이 1.5초라면, 100개의 요청 중 95개는 1.5초 이내에 처리되고 나머지 5개는 더 오래 걸린다는 뜻입니다. 꼬리 지연 시간(Tail Latency) # 꼬리 지연 시간은 상위 백분위에서 나타나는 높은 응답 시간을 의미하며, 이는 사용자 경험에 큰 영향을 미칩니다. 예를 들어, 대부분의 요청이 빠르게 처리되더라도 일부 요청에서 지나치게 긴 응답 시간이 발생하면 사용자는 서비스가 느리다고 느낄 수 있습니다. 따라서 꼬리 지연 시간을 줄이는 것은 고품질 서비스를 제공하는 데 중요한 요소입니다. 효율적 확장을 위한 전략 # 확장성과 관련하여 단순히 자원을 추가하는 방식 외에도 효율적으로 자원을 활용하거나 최적화하는 방법도 중요합니다. 다음과 같은 전략들이 유용할 수 있습니다: 캐싱 전략 강화: 자주 조회되는 데이터를 캐시에 저장하여 데이터베이스 조회 부담을 줄입니다. 데이터베이스 쿼리 최적화: 쿼리 실행 계획을 개선하거나 인덱스를 추가하여 데이터베이스 성능을 향상시킵니다. 로드 밸런싱: 트래픽을 여러 서버로 분산시켜 병목 현상을 방지합니다. 비동기 처리: 시간이 오래 걸리는 작업은 비동기로 처리하여 주요 작업 흐름의 응답 시간을 단축합니다. 또한, 부하와 성능 간 관계를 시각적으로 표현한 그래프나 도표를 활용하면 문제를 분석하고 해결책을 설계하는 데 큰 도움이 됩니다. 위 내용을 바탕으로 확장성과 관련된 논의를 진행하면 보다 체계적이고 실질적인 접근이 가능할 것입니다.","date":"2023-02-05T12:15:15+09:00","href":"https://disj11.github.io/what-you-need-to-know-first-for-scalability/","objectID":"4a0cc8ecd24023d79932941231a36c42_0","order":0,"tags":["development"],"title":"확장성과 성능 최적화: 시스템 부하와 효율적인 대처 전략"},{"content":"Windows 초기 설정 # Windows 최초 설치 후 설정 정리 날개셋 한글 입력기 설치 # 세벌식 유저이므로 날개셋 한글 입력기를 다운받는다. vim 유저이므로 ESC 누를 시 영문으로 전환하는 기능을 설정한다. 설정 방법은 아래 이미지를 참고한다. PowerToys # Microsoft Store 에서 Microsoft PowerToys 를 찾아 설치한다. Scoop # scoop 을 다운 받는다. Set-ExecutionPolicy RemoteSigned -Scope CurrentUser # Optional: Needed to run a remote script the first time irm get.scoop.sh | iex Git # Git 을 다운 받는다. scoop install git VIM # gvim 을 다운받는다. 설치 시 Create .bat files 를 체크한다. 이 옵션을 체크하면 명령 프롬프트에서 vim 명령 사용이 가능해진다. vim-plug 를 설치한다. powershell 을 실행하여 다음 명령어를 입력한다. iwr -useb https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim |` ni $HOME/vimfiles/autoload/plug.vim -Force _vimrc 파일을 작성한다. C:\\Users\\사용자계정 위치에 _vimrc 파일을 생성한 뒤 다음 내용을 입력한다. let mapleader=\u0026#34; \u0026#34; set encoding=UTF-8 set rtp+=/opt/homebrew/opt/fzf set number set showcmd set incsearch set ignorecase set scrolloff=5 set clipboard^=unnamed,unnamedplus set smartindent set expandtab set tabstop=4 set shiftwidth=4 set softtabstop=4 map \u0026lt;Home\u0026gt; ^ map \u0026lt;End\u0026gt; $ nnoremap \u0026lt;leader\u0026gt;ca ggVG \u0026#34; vim-plug call plug#begin() Plug \u0026#39;ryanoasis/vim-devicons\u0026#39; Plug \u0026#39;joshdick/onedark.vim\u0026#39; Plug \u0026#39;vim-airline/vim-airline\u0026#39; Plug \u0026#39;vim-airline/vim-airline-themes\u0026#39; Plug \u0026#39;terryma/vim-multiple-cursors\u0026#39; Plug \u0026#39;preservim/nerdtree\u0026#39; Plug \u0026#39;easymotion/vim-easymotion\u0026#39; Plug \u0026#39;tpope/vim-commentary\u0026#39; Plug \u0026#39;tpope/vim-surround\u0026#39; call plug#end() syntax on colorscheme onedark let g:airline_theme=\u0026#39;onedark\u0026#39; let NERDTreeMapOpenVSplit=\u0026#39;s\u0026#39; let NERDTreeMapPreviewVSplit=\u0026#39;gs\u0026#39; let NERDTreeMapOpenSplit=\u0026#39;S\u0026#39; let NERDTreeMapPreviewSplit=\u0026#39;gS\u0026#39; nnoremap \u0026lt;leader\u0026gt;e :NERDTreeToggle\u0026lt;CR\u0026gt; nnoremap \u0026lt;leader\u0026gt;E :NERDTreeFind\u0026lt;CR\u0026gt; nmap \u0026lt;leader\u0026gt;- :split\u0026lt;CR\u0026gt; nmap \u0026lt;leader\u0026gt;\\| :vsplit\u0026lt;CR\u0026gt; nmap \u0026lt;leader\u0026gt;qq :qall\u0026lt;CR\u0026gt; nmap \u0026lt;leader\u0026gt;/ \u0026lt;Plug\u0026gt;(easymotion-bd-fn) nmap \u0026lt;leader\u0026gt;n \u0026lt;Plug\u0026gt;(easymotion-next) nmap \u0026lt;leader\u0026gt;N \u0026lt;Plug\u0026gt;(easymotion-prev) nmap \u0026lt;leader\u0026gt;j \u0026lt;Plug\u0026gt;(easymotion-j) nmap \u0026lt;leader\u0026gt;k \u0026lt;Plug\u0026gt;(easymotion-k) nmap gsa ysiw nmap gsd ds nmap gsr cs vmap gsa S if has(\u0026#39;gui_running\u0026#39;) set guifont=JetBrainsMono\\ Nerd\\ Font\\ Mono:h13 endif \u0026#34; intellij if has(\u0026#39;ide\u0026#39;) set ideajoin set quickscope nmap \u0026lt;leader\u0026gt;uw \u0026lt;Action\u0026gt;(HideAllWindows) nmap \u0026lt;leader\u0026gt;ft \u0026lt;Action\u0026gt;(ActivateTerminalToolWindow) nmap \u0026lt;leader\u0026gt;* \u0026lt;Action\u0026gt;(FindUsages) nmap \u0026lt;leader\u0026gt;gg \u0026lt;Action\u0026gt;(Git.Branches) nmap gd \u0026lt;Action\u0026gt;(GotoDeclaration) nmap gI \u0026lt;Action\u0026gt;(GotoImplementation) nmap ]m \u0026lt;Action\u0026gt;(MethodDown) nmap [m \u0026lt;Action\u0026gt;(MethodUp) nmap ]e \u0026lt;Action\u0026gt;(GotoNextError) nmap ]E \u0026lt;Action\u0026gt;(GotoPreviousError) nmap \u0026lt;leader\u0026gt;sd \u0026lt;Action\u0026gt;(QuickJavaDoc) nmap \u0026lt;leader\u0026gt;be \u0026lt;Action\u0026gt;(RecentFiles) nmap \u0026lt;leader\u0026gt;ff \u0026lt;Action\u0026gt;(GotoFile) nmap \u0026lt;leader\u0026gt;fn \u0026lt;Action\u0026gt;(NewFile) nmap \u0026lt;leader\u0026gt;sg \u0026lt;Action\u0026gt;(FindInPath) nmap \u0026lt;leader\u0026gt;cf \u0026lt;Action\u0026gt;(ReformatCode) nmap \u0026lt;leader\u0026gt;ca \u0026lt;Action\u0026gt;(Generate) nmap \u0026lt;leader\u0026gt;cr \u0026lt;Action\u0026gt;(RenameElement) nmap \u0026lt;C-n\u0026gt; \u0026lt;Plug\u0026gt;NextWholeOccurrence xmap \u0026lt;C-n\u0026gt; \u0026lt;Plug\u0026gt;NextWholeOccurrence nmap g\u0026lt;C-n\u0026gt; \u0026lt;Plug\u0026gt;NextOccurrence xmap g\u0026lt;C-n\u0026gt; \u0026lt;Plug\u0026gt;NextOccurrence nmap \u0026lt;C-x\u0026gt; \u0026lt;Plug\u0026gt;SkipOccurrence xmap \u0026lt;C-x\u0026gt; \u0026lt;Plug\u0026gt;SkipOccurrence nmap \u0026lt;C-p\u0026gt; \u0026lt;Plug\u0026gt;RemoveOccurrence xmap \u0026lt;C-p\u0026gt; \u0026lt;Plug\u0026gt;RemoveOccurrence nmap \u0026lt;S-C-n\u0026gt; \u0026lt;Plug\u0026gt;AllWholeOccurrences xmap \u0026lt;S-C-n\u0026gt; \u0026lt;Plug\u0026gt;AllWholeOccurrences nmap g\u0026lt;S-C-n\u0026gt; \u0026lt;Plug\u0026gt;AllOccurrences xmap g\u0026lt;S-C-n\u0026gt; \u0026lt;Plug\u0026gt;AllOccurrences endif vim 플러그인을 설치한다. powershell 을 관리자 권한으로 실행한다. vim 명령을 입력 후 vim 으로 진입한다. :PlugInstall 명령어를 통해 플러그인을 설치한다. Nerd Fonts 에서 JetBrainsMono Nerd Font 를 설치한다. 압축 해제하면 여러 개의 파일이 나오는데, JetBrains Mono Regular Nerd Font Complete Mono Windows Compatible 만 설치해도 된다. SDKMAN # 자바 버전 관리를 편하게 하기 위해 sdkman 을 설치한다. scoop bucket add palindrom615 https://github.com/palindrom615/scoop-bucket scoop install sdkman JDK # sdkman 을 사용하여 설치하고 싶은 자바를 선택하여 설치한다. sdk list java 명령어로 자바 버전을 확인할 수 있다. IntelliJ # JetBrains Toolbox App 을 설치한다. 이후 ToolBox App 에서 Intellij IDEA Ultimate 를 설치한다. IntelliJ 설치가 완료되면 아래의 플러그인을 설치한다. IdeaVim IdeaVim-EasyMotion IdeaVim-Quickscope 이외에 필요한 플러그인이 있다면, 추가적으로 설치한다. (Kotest 플러그인 등) 이후 vim 과 동일한 키 설정을 사용하기 하기 위해 .ideavimrc 에 다음 내용을 추가한다. source ~/_vimrc 개발 시 JDK 가 필요하다면 SDK 에 JDK 설치 경로를 추가한다. SDKMAN 으로 설치한 JDK 는 ~/.sdkman 위치에서 찾을 수 있다. Etc. # Hugo # 개인 블로그 개발 환경을 위해 hugo 설치 scoop install hugo-extended","date":"2023-01-30T09:10:13+09:00","href":"https://disj11.github.io/notes/windows-initial-setup/","objectID":"b73afb709007eb214f049c0b08270fc4_0","order":0,"tags":["settings"],"title":"Windows Initial Setup"},{"content":"Kotlin의 Iterable과 Sequence는 컬렉션을 처리하는 두 가지 주요 방식으로, 각기 다른 평가 전략과 성능 특성을 가집니다. 이 글에서는 두 방식의 차이점을 중심으로, 각각의 장단점과 적합한 사용 사례를 설명합니다. Iterable vs Sequence: 평가 방식의 차이 # 1. 즉시 평가 (Eager Evaluation) - Iterable # Iterable은 각 처리 단계에서 전체 컬렉션에 대해 연산을 수행하며, 중간 결과를 새로운 컬렉션으로 반환합니다. 이 방식은 각 단계가 독립적으로 실행되므로 직관적이고 간단하지만, 중간 결과로 인해 메모리 사용량이 증가할 수 있습니다. 작동 방식 # 각 연산(filter, map 등)이 전체 컬렉션에 대해 실행됩니다. 중간 결과로 새로운 리스트가 생성됩니다. 모든 작업이 완료된 후 최종 결과를 얻습니다. 예를 들어, 다음 코드는 Iterable 방식으로 작동합니다: val words = \u0026#34;The quick brown fox jumps over the lazy dog\u0026#34;.split(\u0026#34; \u0026#34;) val lengthsList = words.filter { println(\u0026#34;filter: $it\u0026#34;); it.length \u0026gt; 3 } .map { println(\u0026#34;length: ${it.length}\u0026#34;); it.length } .take(4) println(\u0026#34;Lengths of first 4 words longer than 3 chars:\u0026#34;) println(lengthsList) 출력 결과: filter: The filter: quick filter: brown filter: fox filter: jumps filter: over filter: the filter: lazy filter: dog length: 5 length: 5 length: 5 length: 4 Lengths of first 4 words longer than 3 chars: [5, 5, 5, 4] 위 예제에서 볼 수 있듯이, filter와 map 연산은 모든 요소에 대해 수행되고, 중간 리스트가 생성됩니다. 2. 지연 평가 (Lazy Evaluation) - Sequence # Sequence는 연산을 필요할 때까지 미루며, 각 요소에 대해 연산 체인을 순차적으로 적용합니다. 이는 중간 결과를 저장하지 않으므로 메모리 효율성이 높아지고, 불필요한 계산을 피할 수 있습니다. 작동 방식 # 각 요소가 처리 체인을 통과하며 필요한 만큼만 계산됩니다. 중간 리스트를 생성하지 않습니다. 최종 결과를 요청하는 시점(터미널 연산)에서 계산이 수행됩니다. 다음 코드는 동일한 작업을 Sequence로 처리하는 방법입니다: val words = \u0026#34;The quick brown fox jumps over the lazy dog\u0026#34;.split(\u0026#34; \u0026#34;) val lengthsSequence = words.asSequence() .filter { println(\u0026#34;filter: $it\u0026#34;); it.length \u0026gt; 3 } .map { println(\u0026#34;length: ${it.length}\u0026#34;); it.length } .take(4) println(\u0026#34;Lengths of first 4 words longer than 3 chars:\u0026#34;) println(lengthsSequence.toList()) 출력 결과: Lengths of first 4 words longer than 3 chars: filter: The filter: quick length: 5 filter: brown length: 5 filter: fox filter: jumps length: 5 filter: over length: 4 [5, 5, 5, 4] 여기서 볼 수 있듯이, Sequence는 필요한 만큼만 데이터를 처리하며 불필요한 연산을 피합니다. 성능 비교 # 1. 작은 데이터셋 # 작은 크기의 컬렉션에서는 Iterable이 더 유리할 수 있습니다. 즉시 평가 방식은 CPU 캐시 활용성이 높아 간단한 작업에서는 더 빠르게 동작합니다. 2. 큰 데이터셋 # 큰 데이터셋이나 복잡한 연산 체인에서는 Sequence가 더 적합합니다. 중간 결과를 저장하지 않고 필요한 만큼만 계산하므로 메모리 사용량과 성능 면에서 효율적입니다. 예제 비교 # Iterable: 모든 요소를 처리하고 중간 리스트를 생성. Sequence: 필요한 요소까지만 계산하고 중간 리스트 생성을 피함. 실제 벤치마크에 따르면, 큰 데이터셋에서 Sequence는 수백 배 더 빠른 성능을 보일 수 있습니다. 사용 시점 # 상황 추천 방식 단순하고 작은 데이터셋 Iterable 복잡한 연산 체인 또는 큰 데이터셋 Sequence 중간 결과 저장 필요 Iterable 불필요한 계산 최소화 필요 Sequence 결론 # Kotlin의 Iterable과 Sequence는 각각 다른 방식으로 컬렉션을 처리하며, 적절한 선택은 작업의 특성과 데이터 크기에 따라 달라집니다. 작은 데이터셋이나 간단한 작업에는 Iterable, 메모리 효율성과 성능 최적화가 중요한 경우에는 Sequence를 사용하는 것이 좋습니다. 이러한 차이를 이해하고 적절히 활용하면 더 효율적인 Kotlin 코드를 작성할 수 있습니다. 참고자료: https://kotlinlang.org/docs/sequences.html https://proandroiddev.com/sequences-x-iterable-in-kotlin-b5df65cad2d2?gi=59a6e33d99d9 https://stackoverflow.com/questions/35629159/kotlins-iterable-and-sequence-look-exactly-same-why-are-two-types-required/35630670 https://kt.academy/article/ek-sequence","date":"2023-01-10T23:59:43+09:00","href":"https://disj11.github.io/memory-efficient-iterable-data-processing-with-sequences/","objectID":"b0b2647497d39a8d7b5016846e7ddd3b_0","order":0,"tags":["kotlin"],"title":"Kotlin Iterable과 Sequence의 차이점과 활용법"},{"content":"개요 # 이전까지 자바에서 사용하던 HttpURLConnection 는 지원 수준이 너무 낮아 서드 파티 라이브러리인 Apache HttpClient, Jetty, 스프링의 RestTemplate 을 많이 사용하였다. 하지만 Java 11 에서 HTTP/2와 Web Socket 을 구현하는 HTTP Client API 의 표준화가 정식으로 도입되었다. 이번 포스팅에서는 Java 11 에서 채택된 HTTP Client API 표준화에 대해 알아본다. 변경점 (JEP 321) # Java 9 에서 도입되었던 HTTP API가 이제 공식적으로 Java SE API에 통합 되었다. 새로운 HTTP APIs 는 java.net.HTTP.* 패키지에서 확인할 수 있다. 최신 버전의 HTTP 프로토콜은 stream multiplexing, header compression 와 push promises 등을 통해 전반벅인 성능이 향상되었다. Java 11 부터 API는 비동기로 동작합니다. 비동기는 CompletableFuture 를 사용하여 구현되었다. 새로운 HTTP 클라이언트 API는 외부 종속성 없이 HTTP/2와 같은 최신 웹 기능을 지원한다. 새로운 API는 HTTP 1.1/2 WebSocket에 대한 기본 지원을 제공한다. 핵심 기능을 제공하는 클래스와 인터페이스는 다음과 같다. HttpClient class, java.net.http.HttpClient HttpRequest class, java.net.http.HttpRequest HttpResponse interface, java.net.http.HttpResponse WebSocket interface, java.net.http.WebSocket 이전 버전의 문제점 # 기존에 사용하던 HttpURLConnection API 는 다음과 같은 문제가 존재했다: 더 이상 작동하지 않는 프로토콜을 사용하도록 디자인 되었다. (FTP, gopher, etc.) HTTP/1.1 이전 버전이며 너무 추상적이다. blocking mode 로만 동작한다. (하나의 스레드당 하나의 request/response) HTTP Client API 개요 # HttpURLConnection 과 달리 HTTP Client 는 동기화 비동기 모두를 제공한다. API는 다음의 3가지 핵심 클래스로 이루어 진다. HttpRequest - HttpClient 를 통해 보낼 요청 HttpClient - 여러 요청에 대한 공통 구성 정보를 담는 컨테이너 역할 HttpResponse - HttpRequest 호출의 결과 다음 섹션에서 각각에 대해 더 자세히 알아보자. HttpRequest # 이름에서 알 수 있듯 보내려는 요청을 나타내는 객체이다. HttpRequest.Builder 를 사용하여 새 인스턴스를 만들 수 있다. Builder 클래스는 HttpRequest.newBuilder() 를 통해 얻을 수 있다. URI 설정 # 요청을 생성할 때 가장 먼저 해야 할 일은 URL을 제공하는 것이다. 다음과 같이 두 가지 방법을 통해 URL 을 제공 할 수 있다. HttpRequest.newBuilder(new URI(\u0026#34;https://postman-echo.com/get\u0026#34;)) HttpRequest.newBuilder() .uri(new URI(\u0026#34;https://postman-echo.com/get\u0026#34;)) HTTP Method 지정 # Builder 에서 다음 메서드 중 하나를 호출하여 사용할 HTTP Method 를 정의 할 수 있다. GET() POST(BodyPublisher body) PUT(BodyPublisher body) DELETE() BodyPublisher 에 대해서는 이후에 다루기로 하고, 먼저 간단한 GET 요청 예제를 살펴보자. HttpRequest request = HttpRequest.newBuilder() .uri(new URI(\u0026#34;https://postman-echo.com/get\u0026#34;)) .GET() .build(); 이 요청에는 요청에 필요한 모든 정보가 있다. 하지만 때때로 요청에 추가적인 파라미터가 필요할 수도 있다. 몇 가지 중요한 파라미터에는 다음과 같은 것들이 있다. HTTP protocol 의 버전 headers timeout HTTP protocol 버전 # API 는 기본적으로 HTTP/2 프로토콜을 사용하지만 사용하려는 프로토콜의 버전을 명시할 수 있다. HttpRequest request = HttpRequest.newBuilder() .uri(new URI(\u0026#34;https://postman-echo.com/get\u0026#34;)) .version(HttpClient.Version.HTTP_2) .GET() .build(); 중요한 점은 HTTP/2 가 지원되지 않는 경우 클라이언트는 HTTP/1.1 로 대체한다는 점이다. Header 설정 # header 에 추가적인 정보가 필요한 경우 builder 메서드를 사용할 수 있다. 여기에는 두 가지 방법이 있다. headers() 메서드를 통해 모든 헤더를 키와 값의 쌍으로 제공 header() 메서드를 통해 하나의 키와 값을 제공 HttpRequest request = HttpRequest.newBuilder() .uri(new URI(\u0026#34;https://postman-echo.com/get\u0026#34;)) .headers(\u0026#34;key1\u0026#34;, \u0026#34;value1\u0026#34;, \u0026#34;key2\u0026#34;, \u0026#34;value2\u0026#34;) .GET() .build(); HttpRequest request2 = HttpRequest.newBuilder() .uri(new URI(\u0026#34;https://postman-echo.com/get\u0026#34;)) .header(\u0026#34;key1\u0026#34;, \u0026#34;value1\u0026#34;) .header(\u0026#34;key2\u0026#34;, \u0026#34;value2\u0026#34;) .GET() .build(); Timeout 설정 # Builder 인스턴스의 timeout() 메서드를 사용하여 응답 시간을 설정할 수 있다. 만약 응답 시간을 초과한다면 HttpTimeoutException 이 발생한다. 기본 값은 infinity 이다. HttpRequest request = HttpRequest.newBuilder() .uri(new URI(\u0026#34;https://postman-echo.com/get\u0026#34;)) .timeout(Duration.of(10, SECONDS)) .GET() .build(); Request Body 설정 # Request Method 가 POST, PUT, DELETE 인 경우 요청 본문을 설정할 수 있다. 새로운 API 는 요청 본문을 작성할 수 있도록 여러가지의 BodyPublisher 구현체를 제공한다. StringProcessor ( HttpRequest.BodyPublishers.ofString 를 사용하여 생성함) InputStreamProcessor (HttpRequest.BodyPublishers.ofInputStream 를 사용하여 생성함) ByteArrayProcessor (HttpRequest.BodyPublishers.ofByteArray 를 사용하여 생성함) FileProcessor (HttpRequest.BodyPublishers.ofFile 를 사용하여 생성함) request body 가 필요 없는 경우는 HttpRequest.BodyPublishers.noBody() 를 사용한다. HttpRequest request = HttpRequest.newBuilder() .uri(new URI(\u0026#34;https://postman-echo.com/post\u0026#34;)) .POST(HttpRequest.BodyPublishers.noBody()) .build(); StringBodyPublisher # HttpRequest request = HttpRequest.newBuilder() .uri(new URI(\u0026#34;https://postman-echo.com/post\u0026#34;)) .headers(\u0026#34;Content-Type\u0026#34;, \u0026#34;text/plain;charset=UTF-8\u0026#34;) .POST(HttpRequest.BodyPublishers.ofString(\u0026#34;Sample request body\u0026#34;)) .build(); InputStreamBodyPublisher # byte[] sampleData = \u0026#34;Sample request body\u0026#34;.getBytes(); HttpRequest request = HttpRequest.newBuilder() .uri(new URI(\u0026#34;https://postman-echo.com/post\u0026#34;)) .headers(\u0026#34;Content-Type\u0026#34;, \u0026#34;text/plain;charset=UTF-8\u0026#34;) .POST(HttpRequest.BodyPublishers.ofInputStream(() -\u0026gt; new ByteArrayInputStream(sampleData))) .build(); ByteArrayProcessor # byte[] sampleData = \u0026#34;Sample request body\u0026#34;.getBytes(); HttpRequest request = HttpRequest.newBuilder() .uri(new URI(\u0026#34;https://postman-echo.com/post\u0026#34;)) .headers(\u0026#34;Content-Type\u0026#34;, \u0026#34;text/plain;charset=UTF-8\u0026#34;) .POST(HttpRequest.BodyPublishers.ofByteArray(sampleData)) .build(); FileProcessor # HttpRequest request = HttpRequest.newBuilder() .uri(new URI(\u0026#34;https://postman-echo.com/post\u0026#34;)) .headers(\u0026#34;Content-Type\u0026#34;, \u0026#34;text/plain;charset=UTF-8\u0026#34;) .POST(HttpRequest.BodyPublishers.fromFile( Paths.get(\u0026#34;src/test/resources/sample.txt\u0026#34;))) .build(); HttpClient # 모든 요청은 HttpClient 를 통해 전송한다. HttpClient 는 HttpClient.newBuilder() 메서드 또는 HttpClient.newHttpClient() 를 통해 얻을 수 있다. Response Body 핸들링 # Publisher 와 비슷하게 Response Body handler 생성을 위한 메서드가 있다. BodyHandlers.ofByteArray BodyHandlers.ofString BodyHandlers.ofFile BodyHandlers.discarding BodyHandlers.replacing BodyHandlers.ofLines BodyHandlers.fromLineSubscriber BodyHandlers 팩토리 클래스의 사용에 주의한다. java 11 이전 버전에서는 다음과 같이 사용했다. HttpResponse\u0026lt;String\u0026gt; response = client.send(request, HttpResponse.BodyHandler.asString()); 이제는 다음과 같이 사용할 수 있다. HttpResponse\u0026lt;String\u0026gt; response = client.send(request, BodyHandlers.ofString()); Proxy 설정 # Builder 인스턴스에서 proxy() 메서드를 사용하여 간단하게 프록시를 추가할 수 있다. 다음은 시스템 기본 프록시를 사용하게 하는 예제이다. HttpResponse\u0026lt;String\u0026gt; response = HttpClient .newBuilder() .proxy(ProxySelector.getDefault()) .build() .send(request, BodyHandlers.ofString()); Redirect Polcy 설정 # 접근하려는 페이지가 다른 주소로 이동하는 경우가 있다. 이 경우 일반적으로 변경된 URI 와 함께 HTTP 상태코드 3xx 를 받게 된다. 적절한 리다이렉션 정책을 설정하면 HttpClient 가 자동으로 요청을 새 URI 로 리다이렉션 한다. 리다이렉션 정책 설정은 Builder 인스턴스의 followRedirects() 메서드를 사용한다. HttpResponse\u0026lt;String\u0026gt; response = HttpClient.newBuilder() .followRedirects(HttpClient.Redirect.ALWAYS) .build() .send(request, BodyHandlers.ofString()); 리다이렉션 정책은 HttpClient.Redirect 에 정의 되어 있다. Authenticator 설정 # Authenticator 는 연결을 위한 자격증명을 나타낸다. 예를 들어 연결 하려는 서버가 username, password","date":"2021-11-02T18:11:57+09:00","href":"https://disj11.github.io/http-client-in-java/","objectID":"c92630f9f1d66213ba2997c1abb859dc_0","order":0,"tags":["java"],"title":"Http Client in Java"},{"content":"를 요구한다면 PasswordAuthentication 클래스를 사용할 수 있다. HttpResponse\u0026lt;String\u0026gt; response = HttpClient.newBuilder() .authenticator(new Authenticator() { @Override protected PasswordAuthentication getPasswordAuthentication() { return new PasswordAuthentication( \u0026#34;username\u0026#34;, \u0026#34;password\u0026#34;.toCharArray()); } }).build() .send(request, BodyHandlers.ofString()); Send Requests - Sync vs. Async # HttpClient 는 동기와 비동기 요청 모두 제공한다. send() - 동기 sendAsync() - 비동기 send 메서드는 응답이 올 때까지 기다리고, HttpResponse 객체를 리턴한다. HttpResponse\u0026lt;String\u0026gt; response = HttpClient.newBuilder() .build() .send(request, BodyHandlers.ofString()); 응답이 올 때까지 기다리기 때문에 많은 양의 데이터를 처리해야 할 때 단점이 있다. 반면에 sendAsync 메서드는 비동기로 작동하며 CompletableFeature\u0026lt;HttpResponse\u0026gt; 를 리턴한다. CompletableFuture\u0026lt;HttpResponse\u0026lt;String\u0026gt;\u0026gt; response = HttpClient.newBuilder() .build() .sendAsync(request, HttpResponse.BodyHandlers.ofString()); 다음과 같은 사용도 가능하다. List\u0026lt;URI\u0026gt; targets = Arrays.asList( new URI(\u0026#34;https://postman-echo.com/get?foo1=bar1\u0026#34;), new URI(\u0026#34;https://postman-echo.com/get?foo2=bar2\u0026#34;)); HttpClient client = HttpClient.newHttpClient(); List\u0026lt;CompletableFuture\u0026lt;String\u0026gt;\u0026gt; futures = targets.stream() .map(target -\u0026gt; client .sendAsync( HttpRequest.newBuilder(target).GET().build(), HttpResponse.BodyHandlers.ofString()) .thenApply(response -\u0026gt; response.body())) .collect(Collectors.toList()); 비동기를 위한 Executor 설정 # 비동기 호출 시 사용할 스레드를 제공하는 Executor 을 정의할 수도 있다. 이를 사용하여 요청 처리에 사용되는 스레드 수를 제한할 수 있다. ExecutorService executorService = Executors.newFixedThreadPool(2); CompletableFuture\u0026lt;HttpResponse\u0026lt;String\u0026gt;\u0026gt; response1 = HttpClient.newBuilder() .executor(executorService) .build() .sendAsync(request, HttpResponse.BodyHandlers.ofString()); CompletableFuture\u0026lt;HttpResponse\u0026lt;String\u0026gt;\u0026gt; response2 = HttpClient.newBuilder() .executor(executorService) .build() .sendAsync(request, HttpResponse.BodyHandlers.ofString()); HttpClient 는 기본적으로 java.util.concurrent.Executors.newCachedThreadPool() 를 사용한다. CookieHandler 설정 # Builder 의 cookieHandler 메서드를 사용하여 클라이언트별 CookieHandler 를 손쉽게 설정할 수 있다. 예를 들어 모든 쿠키를 허용하지 않으려면 다음과 같이 사용할 수 있다. HttpClient.newBuilder() .cookieHandler(new CookieManager(null, CookiePolicy.ACCEPT_NONE)) .build(); 만약 CookieManager 가 쿠키 저장을 허용했다면 HttpClient 에서 CookieHandler 를 통해 쿠키에 액세스 할 수 있다. ((CookieManager) httpClient.cookieHanlder().get()).getCookieStore() HttpResponse # HttpResponse 클래스는 서버의 응답을 나타낸다. 여러가지 유용한 메서드가 있지만 가장 중요한 것은 두 가지 이다. statusCode() - HTTP 상태 코드를 반환한다. body() - 응답에 대한 본문을 반환하며 반환 유형은 send() 메서드에 전달된 BodyHandler 에 따라 다르다. 이 외에도 uri(), headers(), trailers(), version() 등과 같은 유용한 메서드가 있다. Response 객체의 URI # Response 객체의 uri() 메서드는 응답 된 URI 를 반환한다. 리다이렉션이 일어났을 수도 있기 때문에 request 객체의 URI 와 다른 경우도 있다. assertThat(request.uri().toString(), equalTo(\u0026#34;http://stackoverflow.com\u0026#34;)); assertThat(response.uri().toString(), equalTo(\u0026#34;https://stackoverflow.com/\u0026#34;)); Response Headers # Response 객체의 headers() 메서드를 통해 응답 헤더를 확인할 수 있다. 응답 헤더는 HttpHeaders 이며 read-only 이다. HttpResponse\u0026lt;String\u0026gt; response = HttpClient.newHttpClient() .send(request, HttpResponse.BodyHandlers.ofString()); HttpHeaders responseHeaders = response.headers(); Response Version # version() 메서드를 통해 서버와 통신하는 데 사용된 HTTP 프로토콜의 버전을 알 수 있다. 요청 시 HTTP/2 버전을 사용했더라도 HTTP/1.1 을 통해 응답이 올 수도 있음에 주의한다. HttpRequest request = HttpRequest.newBuilder() .uri(new URI(\u0026#34;https://postman-echo.com/get\u0026#34;)) .version(HttpClient.Version.HTTP_2) .GET() .build(); HttpResponse\u0026lt;String\u0026gt; response = HttpClient.newHttpClient() .send(request, HttpResponse.BodyHandlers.ofString()); assertThat(response.version(), equalTo(HttpClient.Version.HTTP_1_1)); 참조 # Exploring the New HTTP Client in Java","date":"2021-11-02T18:11:57+09:00","href":"https://disj11.github.io/http-client-in-java/","objectID":"c92630f9f1d66213ba2997c1abb859dc_1","order":1,"tags":["java"],"title":"Http Client in Java"},{"content":"개요 # DecimalFormat은 미리 정의된 포맷을 사용하여 10진수 문자열 표현을 형식화 할 수 있는 NumberFormat 의 하위 클래스이다. 역으로 문자열을 숫자로 구문 분석하는 데 사용할 수도 있다. 이번 포스팅에서는 DecimalFormat 의 사용법을 알아본다. 패턴 문자 # 숫자를 어떤 형식으로 나타낼 지 지정하기 위해서는 먼저 패턴 문자를 알아야한다. 총 11가지 문자가 있지만, 네 가지만 알고 있으면 대부분의 상황에서 문제 없이 사용할 수 있다. 0 : 값이 제공되면 숫자를, 그렇지 않다면 0을 출력 # : 값이 제공되면 숫자를, 그렇지 않다면 아무것도 출력하지 않음 . : 소수점 구분 기호를 넣을 위치를 지정 , : 그룹화 구분 기호를 넣을 위치를 지정 DecimalFormat 사용 시 패턴이 지정될 경우 지정된 규칙이 실행되고, 그렇지 않은 경우는 JVM Locale 의 DecimalFormatSymbol 에 따라 규칙이 실행된다. 기본 포맷팅 # 실제 코드를 통해 포맷을 적용해보자. Simple Decimal # 정수 부분은 패턴 문자가 입력되는 문자보다 개수가 더 적더라도 잘리지 않는다. 다음의 테스트 코드를 통해 확인할 수 있다. double d = 123.45; Assertions.assertEquals(new DecimalFormat(\u0026#34;#.##\u0026#34;).format(d), \u0026#34;123.45\u0026#34;); Assertions.assertEquals(new DecimalFormat(\u0026#34;0.00\u0026#34;).format(d), \u0026#34;123.45\u0026#34;); 정수와 소수 부분의 패턴 문자가 입력되는 문자보다 길 경우는 # 인 경우 삭제되고, 0 인 경우는 0 이 채워지는 것을 볼 수 있다. double d = 123.45; assertEquals(new DecimalFormat(\u0026#34;####.###\u0026#34;).format(d), \u0026#34;123.45\u0026#34;); assertEquals(new DecimalFormat(\u0026#34;0000.000\u0026#34;).format(d), \u0026#34;0123.450\u0026#34;); Rounding (반올림) # 앞에서 정수 부분은 패턴 문자가 입력되는 문자보다 개수가 더 적더라도 잘리지 않는다고 하였다. 하지만 소수 부분은 패턴 문자가 더 적은 경우, 패턴 문자의 길이에 맞게 반올림 된다. double d = 123.45; assertEquals(new DecimalFormat(\u0026#34;#.#\u0026#34;).format(d), \u0026#34;123.5\u0026#34;); assertEquals(new DecimalFormat(\u0026#34;#\u0026#34;).format(d), \u0026#34;123\u0026#34;); Grouping (그룹핑) # , 패턴 문자는 다음과 같이 사용한다. double d = 1234567.89; assertEquals(new DecimalFormat(\u0026#34;#,###.#\u0026#34;).format(d), \u0026#34;1,234,567.9\u0026#34;); assertEquals(new DecimalFormat(\u0026#34;#,###\u0026#34;).format(d), \u0026#34;1,234,568\u0026#34;); Mixing String Literals (문자열 리터럴 혼용) # 문자열 리터럴과 패턴을 혼용하여 사용할 수 있다. double d = 1234567.89; assertEquals(new DecimalFormat(\u0026#34;The # number\u0026#34;).format(d), \u0026#34;The 1234568 number\u0026#34;); 다음과 같은 방법을 통해 문자열 리터럴에 특수 문자를 사용 수도 있다. double d = 1234567.89; assertEquals(new DecimalFormat(\u0026#34;The \u0026#39;#\u0026#39; # number\u0026#34;).format(d), \u0026#34;The # 1234568 number\u0026#34;); Localized Formatting # 이탈리아 같은 몇몇의 나라에서는 그룹핑 문자로 . 를 사용하고 소수 구분 기호로 , 를 사용한다. 이런 나라에서는 #,###.## 패턴을 이용할 시 1.234.567,89 로 포맷팅 된다. 경우에 따라 이는 유용한 i18n 기능이 될 수도 있지만, 그렇지 않은 경우도 있을 것이다. 이럴 때에는 DecimalFormatSymbols 을 사용한다. double d = 1234567.89; assertEquals(new DecimalFormat(\u0026#34;#,###.##\u0026#34;, new DecimalFormatSymbols(Locale.ENGLISH)).format(d), \u0026#34;1,234,567.89\u0026#34;); assertEquals(new DecimalFormat(\u0026#34;#,###.##\u0026#34;, new DecimalFormatSymbols(Locale.ITALIAN)).format(d), \u0026#34;1.234.567,89\u0026#34;); Parsing # 다음과 같이 문자열을 숫자로 파싱이 가능하다. assertEquals(new DecimalFormat(\u0026#34;\u0026#34;, new DecimalFormatSymbols(Locale.ENGLISH)).parse(\u0026#34;1234567.89\u0026#34;), 1234567.89); Thread-Safety # DecimalFormat 은 스레드에 안전하지 않기 때문에 스레드 간 동일 인스턴스를 공유할 때 주의하여야 한다. 참조 # A Practical Guide to DecimalFormat","date":"2021-11-01T20:00:12+09:00","href":"https://disj11.github.io/number-formatter-in-java/","objectID":"2c1f8384688be7e39ddc4bee18838b0b_0","order":0,"tags":["java"],"title":"Number Formatter in Java"},{"content":"소개 # OAuth는 오픈 API의 인증(authentication)과 권한 부여(authorization)를 제공하기 위해 만들어진 프로토콜이다. OAuth 1.0과 OAuth 2.0이 있는데, 현재는 RFC 5849에서 설명하는 OAuth 1.0을 폐기하고, RFC 6749에 설명된 OAuth 2.0 방식을 사용한다. 이번 포스팅에서는 OAuth 2.0에 관하여 알아본다. 역할 (Rules) # OAuth 2.0을 이해하기 위해서는 먼저 OAuth 2.0에서 정의하는 4가지 역할에 관하여 알아야한다. Resource Owner (리소스 소유자) Resource Service (리소스 서버) Client (클라이언트) Authorization Server (인증 서버) 리소스 소유자는 보호된 리소스의 소유자를 말한다. 예를 들어 은행관련 서비스에서 계좌 잔액이라는 리소스가 있다면, 이 계좌의 소유주(예금주)가 리소스의 소유자가 된다. 리소스 서버는 보호된 리소스를 제공하는 서버를 말한다. 예를 들어 오픈뱅킹 서비스는 각 은행들의 API를 연동하여 다양한 리소스(거래내역, 계좌실명 등)를 제공한다. 이런 경우 오픈뱅킹의 서버가 리소스 서버라고 할 수 있다. 클라이언트는 오픈 API를 호출하는 응용 프로그램을 말한다. 예를 들어 오픈뱅킹 API를 이용하여 모든 은행들의 잔액을 볼 수 있는 어플리케이션을 만들었다면, 이 어플리케이션이 클라이언트가 된다. 인증 서버는 리소스 소유자로부터 리소스에 접근 권한을 획득한 이후에 리소스에 접근하기 위한 엑세스 토큰(Access Token)을 발급해주는 서버를 말한다. OAuth 2.0의 흐름 # 다음 그림은 OAuth 2.0의 대략적인 흐름을 나타낸다. +--------+ +---------------+ | |--(1)- Authorization Request -\u0026gt;| Resource | | | | Owner | | |\u0026lt;-(2)-- Authorization Grant ---| | | | +---------------+ | | | | +---------------+ | |--(3)-- Authorization Grant --\u0026gt;| Authorization | | Client | | Server | | |\u0026lt;-(4)----- Access Token -------| | | | +---------------+ | | | | +---------------+ | |--(5)----- Access Token ------\u0026gt;| Resource | | | | Server | | |\u0026lt;-(6)--- Protected Resource ---| | +--------+ +---------------+ 그림을 보고 하나씩 짚어보자. 리소스 서버에게 리소스를 요청하기 전에 먼저 인증을 요청한다. 인증 요청은 리소스 소유자에게 직접 할 수도 있지만, 중간에 인증 서버를 통해 간접적으로 하는 것이 좋다. 클라이언트는 리소스 소유자의 인가를 나타내는 자격정보인 인가 승인을 받는다. 클라이언트는 2번에서 받은 인가 승인을 사용하여 엑세스 토큰을 요청한다. 인증 서버는 클라이언트를 인증하고, 제시된 인가 승인이 유효한지 확인한다. 유효한 경우 엑세스 토큰을 발급한다. 클라이언트는 엑세스 토큰을 제시하여 리소스 서버에 보호된 리소스를 요청한다. 리소스 서버는 엑세스 토큰이 유효한지 확인하고, 유효한 경우 요청을 받아들인다. 인가 승인 (Authorization Grant) # \u0026ldquo;인가 승인\u0026quot;은 리소스 소유자가 보호된 리소스에 대한 접근을 허용한다는 것을 나타내는 자격 정보(Credentials)이다. 이는 클라이언트가 엑세스 토큰을 얻기 위해 사용된다. 인가 승인에는 네 가지 유형이 있다. 이번 포스팅에서는 네 가지 유형 중 인가 코드(authorization code) 방식에 대해서만 설명하며, 추가적인 유형은 RFC 6749 - Authorization Grant를 참고한다. 인가 코드 (Authorization Code) # 클라이언트는 사용자 에이전트(User-Agent)를 통해 리소스 소유자를 인증 서버로 안내한다. 인증 서버는 리소스 소유자를 인증하고, 인증이 완료되면 클라이언트는 인증 코드를 획득한다. 네이버 로그인이나 카카오 로그인 API가 사용하는 인가 승인 유형이 바로 이 유형이다. 엑세스 토큰 (Access Token) # 엑세스 토큰은 보호된 리소스에 접근할 수 있도록 하는 권한 증명이다. 엑세스 토큰으로 접근할 수 있는 리소스의 범위와 사용할 수 있는 기간이 정해져 있다. 리프레시 토큰 (Refresh Token) # 리프레시 토큰은 엑세스 토큰을 얻는 데 사용할 수 있는 권한 증명이다. 엑세스 토큰이 유효하지 않거나, 만료된 경우 새로운 엑세스 토큰을 발급 받기 위해 사용된다. 더 좁은 범위로 추가적인 엑세스 토큰을 받기 위해 사용되기도 한다. 아래의 그림은 리프레시 토큰을 사용하여 엑세스 토큰을 갱신하는 흐름을 보여준다. +--------+ +---------------+ | |--(1)------- Authorization Grant ---------\u0026gt;| | | | | | | |\u0026lt;-(2)----------- Access Token -------------| | | | \u0026amp; Refresh Token | | | | | | | | +----------+ | | | |--(3)---- Access Token ----\u0026gt;| | | | | | | | | | | |\u0026lt;-(4)- Protected Resource --| Resource | | Authorization | | Client | | Server | | Server | | |--(5)---- Access Token ----\u0026gt;| | | | | | | | | | | |\u0026lt;-(6)- Invalid Token Error -| | | | | | +----------+ | | | | | | | |--(7)----------- Refresh Token -----------\u0026gt;| | | | | | | |\u0026lt;-(8)----------- Access Token -------------| | 인가 획득 (Obtaining Authorization) # 클라이언트가 리소스 소유자로부터 인가를 획득하는 과정에 대해 알아보자. 인가 승인에서 알아본 것처럼 인가 획득을 위한 유형에는 네 가지가 있지만, 이 포스팅에서는 인가 코드를 사용하여 인가를 획득하는 과정에 대해서만 설명한다. 인가 코드 승인 유형은 리다이렉션이 기반이 된다. 때문에 클라이언트는 리소스 소유자의 유저 에이전트(일반적으로 웝 브라우저)와 상호작용 할 수 있어야하며, 리다이렉션을 통해 인증 서버로 오는 요청을 받을수 있어야 한다. 인가 코드 승인 유형 흐름 # 다음 그림은 인가 코드 방식의 흐름을 보여준다. +----------+ | Resource | | Owner | | | +----------+ ^ | (2) +----|-----+ Client Identifier +---------------+ | -+----(1)-- \u0026amp; Redirection URI ----\u0026gt;| | | User- | | Authorization | | Agent -+----(2)-- User authenticates ---\u0026gt;| Server | | | | | | -+----(3)-- Authorization Code ---\u0026lt;| | +-|----|---+ +---------------+ | | ^ v (1) (3) | | | | | | ^ v | | +---------+ | | | |\u0026gt;---(4)-- Authorization Code ---------\u0026#39; | | Client | \u0026amp; Redirection URI | | | | | |\u0026lt;---(5)----- Access Token -------------------\u0026#39; +---------+ (w/ Optional Refresh Token) 1,2,3번의 과정이 두 부분으로 나누어 지는데, 이는 유저 에이전트를 통해 전달되기 때문이다. 1번은 클라이언트가 리소스 소유자의 유저 에이전트를 인증 서버로 안내하며 흐름을 시작하는 과정이다. 클라이언트는 클라이언트의 식별자(client identifier),","date":"2021-10-22T10:08:30+09:00","href":"https://disj11.github.io/an-introduction-to-oauth2/","objectID":"a177d88fa4b2236fde560a5c14b1fe69_0","order":0,"tags":["development"],"title":"Oauth2 에 대해서 알아보자"},{"content":"요청 범위(requested scope), 로컬 상태(local state), 리다이렉션 URI를 포함해야한다. 네이버 아이디로 로그인 API를 연동한 어플리케이션에서 [네이버 아이디로 로그인] 버튼을 누르면 흐름이 시작되는데, 이 과정이라고 생각하면 된다. 2번은 인증 서버가 유저 에이전트를 통해 리소스 소유자를 인증하고, 리소스 소유자가 클라이언트의 접근 요청에 승인 혹은 거부할 지를 선택하는 단계이다. 3번 과정은 인증 서버가 유저 에이전트를 제공된 리다이렉션 URI로 이동시키는 과정이다. 이때 리다이렉션 URI에는 인증 코드와 로컬 상태가 포함된다. 4번은 클라이언트가 이전 과정에서 받은 인증 코드를 포함하여 인증 서버에 엑세스 토큰을 요청하는 단계이다. 요청을 보낼 때에는 3번 과정에서 사용했던 리다이렉션 URI도 포함하여 전달한다. 5번 과정에서 인증서버는 인증 코드가 유효한지 확인하고, 3번 과정과 4번 과정의 리다이렉션 URI가 동일한지 확인한다. 유효하다는 게 획인되면 엑세스 토큰과 (선택적으로) 리프레시 토큰을 응답한다. 인가 요청 (Authorization Request) # 클라이언트는 인증 코드 요청을 할 때 application/x-www-form-urlencoded 를 사용하여 다음 파라미터를 포함해야한다. response_type 필수. \u0026ldquo;code\u0026quot;로 고정 client_id 필수. 클라이언트의 식별자 redirect_uri 선택사항. 자세한 내용은 RFC 6749 - Redirection Endpoint 참고 scope 선택사항. 자세한 내용은 RFC 6749 - Access Token Scope 참고 state 권장사항. 인증 서버는 유저 에이전트를 클라이언트로 리다이렉트할 때 이 값을 포함한다. RFC 6749 - Cross-Site Request Forgery에 기술된대로 사이트 간 요청 위조를 방지하는 데 사용하는 것이 좋다. 인가 요청의 예: GET /authorize?response_type=code\u0026amp;client_id=s6BhdRkqt3\u0026amp;state=xyz\u0026amp;redirect_uri=https%3A%2F%2Fclient%2Eexample%2Ecom%2Fcb HTTP/1.1 Host: server.example.com 인가 응답 (Authorization Response) # 리소스 소유자가 접근 요청을 승인하면, 인증 서버는 다음의 파라미터를 application/x-www-form-urlencoded 를 사용하여 클라이언트에게 전달한다. 성공 응답 (Successful Response) # code 필수. 인증 서버에서 생성된 인증 코드이다. 유출 위험을 줄이기 위해 만료 시간이 짧아야 하며 최대 10분이 권장된다. 클라이언트는 인증 코드를 두 번 이상 사용하면 안된다. 만약 두번 이상 사용될 경우 인증 서버는 요청을 거부해야 하며, 해당 인증 코드 이전에 발급된 모든 토큰을 취소하는 것이 좋다. state 클라이언트가 인증 요청 시 state 파라미터를 포함했다면 필수. 클라이언트로부터 전달 받은 값과 동일해야한다. 성공 응답의 예: HTTP/1.1 302 Found Location: https://client.example.com/cb?code=SplxlOBeZQQYbYS6WxSbIA\u0026amp;state=xyz 오류 응답 (Error Response) # 만약 요청이 누락되거나, 유효하지 않는 경우, 리다이렉션 URI가 일치하지 않는 경우, 클라이언트 식별자가 누락되거나 유효하지 않는 경우는 유저 에이전트를 유효하지 않은 URI로 리다이렉트 되게 해서는 안된다. 이 경우에 인증 서버는 다음 파라미터를 사용하여 리소스 소유자에게 오류를 알려주는 것이 좋다. error 필수. 다음의 에러 코드 사용 invalid_request 요청 시 필수 파라미터의 누락, 유효하지 않은 파라미터 포함, 파라미터를 두 번 이상 포함 등 unauthorized_client 클라이언트가 이 인가 승인 유형을 사용할 권한이 없음 access_denied 리소스 소유자 또는 인증 서버가 요청을 거부 unsupported_response_type 인증 서버가 Authorization Code Grant 유형을 지원하지 않음 invalid_scope 요청한 scope가 유효하지 않거나, 알 수 없거나, 손상된 경우 server_error 인증 서버에 예기치 못한 오류가 발생 temporarily_unavailable 인증 서버의 과부하 또는 유지보수로 인해 요청을 처리할 수 없음 error_description 선택사항. 클라이언트 개발자가 발생한 오류를 이해하는 데 도움을 주는 정보를 제공한다. error_uri 선택사항. 클라이언트 개발자를 위해 발생한 오류와 관련된 추가 정보를 제공한다. state 클라이언트가 인증 요청 시 state 파라미터를 포함했다면 필수. 클라이언트로부터 전달 받은 값과 동일해야한다. 오류 응답의 예: HTTP/1.1 302 Found Location: https://client.example.com/cb?error=access_denied\u0026amp;state=xyz 엑세스 토큰 요청 (Access Token Request) # 클라이언트는 토큰 요청시 UTF-8 인코딩을 사용하여 application/x-www-form-urlencoded 형식으로 된 다음과 같은 파라미터를 body에 담아야한다. grant_type 필수. 값은 \u0026ldquo;authorization_code\u0026quot;로 고정 code 필수. 인증 서버로부터 받은 인증 코드 redirect_uri 인증 요청 시 redirect_uri가 존재했다면 필수이며, 인증 요청 시 사용했던 값과 동일해야 한다. client_id 클라이언트가 인증 서버와 인증하지 않는 경우 필수. 클라이언트 인증에 관한 자세한 내용은 RFC 6749 - Client Authentication를 참고한다. 토큰 요청의 예: POST /token HTTP/1.1 Host: server.example.com Authorization: Basic czZCaGRSa3F0MzpnWDFmQmF0M2JW Content-Type: application/x-www-form-urlencodedgrant_type=authorization_code\u0026amp;code=SplxlOBeZQQYbYS6WxSbIA\u0026amp;redirect_uri=https%3A%2F%2Fclient%2Eexample%2Ecom%2Fcb 엑세스 토큰 응답 (Access Token Response) # Access Token 요청이 유효하고 인증되었다면, 인증 서버는 Access Token과 선택적으로 갱신 토큰을 발급한다. 성공 응답 (Successful Response) # 인증 서버는 Access Token과 선택적으로 갱신 토큰을 발급하고, 다음 파라미터를 200 (OK) 상태 코드로 응답한다. 파라미터는 application/json 유형을 사용하여 HTTP Response Body에 포함한다. access_token 필수. 인증 서버가 발급한 엑세스 토큰 token_type 필수. 토큰의 타입으로 보통 baerer 타입을 많이 사용한다. 자세한 내용은 RFC 6749 - Access Token Types를 참고한다. expires_in 권장사항. 초 단위의 Access Token 수명. 예를 들어 값이 3600이라면 토큰이 생성된 시간으로부터 3600초(1 시간) 뒤에 만료된다는 의미이다. refresh_token 선택사항. 인증 서버가 발급한 리프레시 토큰 scope 클라이언트가 요청한 범위와 동일하다면 선택사항. 그렇지 않은 경우 필수. 자세한 내용은 RFC 6749 - Access Token Scope를 참고한다. 성공 응답의 예: HTTP/1.1 200 OK Content-Type: application/json;charset=UTF-8 Cache-Control: no-store Pragma: no-cache { \u0026#34;access_token\u0026#34;:\u0026#34;2YotnFZFEjr1zCsicMWpAA\u0026#34;, \u0026#34;token_type\u0026#34;:\u0026#34;example\u0026#34;, \u0026#34;expires_in\u0026#34;:3600, \u0026#34;refresh_token\u0026#34;:\u0026#34;tGzv3JOkF0XG5Qx2TlKWIA\u0026#34;, \u0026#34;example_parameter\u0026#34;:\u0026#34;example_value\u0026#34; } 오류 응답 (Error Response) # 인증 서버는 다음 파라미터를 400 (Bad Request) 상태 코드로 응답한다. 파라미터는 application/json 유형을 사용하여 HTTP Response Body에 포함한다. error 필수. 다음의 에러 코드 사용 invalid_request 요청 시 필수 파라미터의 누락, 유효하지 않은 파라미터 포함, 파라미터를 두 번 이상 포함 등 invalid_client 클라이언트가 인증에 실패한 경우 (e.g. 알 수 없는 클라이언트, 클라이언트 인증이 포함되지 않음, 지원되지 않는 인증 방법). invalid_grant 인가 승인 유형 또는 Refresh Token이 유효하지 않거나, 만료, 취소된 경우 또는 인증 요청에 사용된 리다이렉션 URI가 일치하지 않거나 다른 클라이언트에게 발급된 경우 unauthorized_client 클라이언트가 이 인가 승인 유형을 사용할 권한이 없음 access_denied 리소스 소유자 또는 인증 서버가 요청을 거부 unsupported_grant_type 서버가 지원하지 않는 인가 승인 유형인 경우 temporarily_unavailable 인증 서버의 과부하 또는 유지보수로 인해 요청을 처리할 수 없음 (503 Service Unavailable 상태 코드는 HTTP redirect를 통해 클라이언트에게","date":"2021-10-22T10:08:30+09:00","href":"https://disj11.github.io/an-introduction-to-oauth2/","objectID":"a177d88fa4b2236fde560a5c14b1fe69_1","order":1,"tags":["development"],"title":"Oauth2 에 대해서 알아보자"},{"content":"전달 될 수 없기 때문에 이 오류 코드가 필요) invalid_scope 요청한 scope가 유효하지 않거나, 알 수 없거나, 손상된 경우 error_description 선택사항. 클라이언트 개발자가 발생한 오류를 이해하는 데 도움을 주는 정보를 제공한다. error_uri 선택사항. 클라이언트 개발자를 위해 발생한 오류와 관련된 추가 정보를 제공한다. 오류 응답의 예: HTTP/1.1 400 Bad Request Content-Type: application/json;charset=UTF-8 Cache-Control: no-store Pragma: no-cache { \u0026#34;error\u0026#34;:\u0026#34;invalid_request\u0026#34; } 사용 예 # 카카오와 네이버에서도 OAuth 2.0 방식의 로그인 API를 제공한다. 카카오 # 카카오 로그인 문서를 확인해보면, 카카오 로그인을 위해서 인가 코드 받기, 토큰 받기 두 과정을 거친다. 이 과정이 인가 획득과 동일하다. 이렇게 발급받은 토큰은 카카오스토리 API 등을 사용할 때 필요하다. 예를 들어 카카오스토리의 프로필 가져오기 API를 사용하려면 이 발급받은 토큰이 필요하다. 이 과정이 OAuth 2.0의 흐름의 5,6번 과정에 속한다. 네이버 # 네이버도 OAuth를 이용하며, 네이버 로그인 API 명세에서 확인할 수 있다. 카카오와 마찬가지로 인가 코드 받기, 토큰 받기 두 과정을 거친다. 이렇게 발급받은 토큰을 사용하여 회원 프로필 조회 API, 카페 API 등을 사용할 수 있다. 마무리 # 이 포스팅은 OAuth 2.0의 클라이언트 구현에 도움이 되는 내용에 초점을 맞춰 생략된 부분이 많다. 만약 OAuth 2.0에 대해 더 자세히 알고 싶다면, RFC 6749을 참고하는 것이 좋다.","date":"2021-10-22T10:08:30+09:00","href":"https://disj11.github.io/an-introduction-to-oauth2/","objectID":"a177d88fa4b2236fde560a5c14b1fe69_2","order":2,"tags":["development"],"title":"Oauth2 에 대해서 알아보자"},{"content":"개요 # Java8 에서 추가된 DateTimeFormatter 클래스에 대해 알아보자. 미리 정의된 인스턴스 # DateTimeFormatter 에는 ISO 및 RFC 표준을 따라 정의되어 있는 날짜/시간 포맷을 제공한다. 예를들어 ISO_LOCAL_DATE 인스턴스를 사용하여 다음과 같이 \u0026lsquo;2021-09-29\u0026rsquo; 와 같은 문자열을 얻을 수 있다. LocalDate date = LocalDate.of(2021, 9, 29); DateTimeFormatter.ISO_LOCAL_DATE.format(date); // 2021-09-29 만약 \u0026lsquo;2021-09-29+09:00\u0026rsquo; 와 같이 오프셋을 포함한 문자열을 구하고 싶다면 ISO_OFFSET_DATE 를 사용한다. LocalDate date = LocalDate.of(2021, 9, 29); DateTimeFormatter.ISO_OFFSET_DATE.format(date.atStartOfDay(ZoneId.of(\u0026#34;UTC+9\u0026#34;))); // 2021-09-29+09:00 FormatStyle의 사용 # 사람이 이해하기 쉽게 날짜를 보여주고 싶을때가 있다. 이럴 때에는 java.time.format.FormatStyle 을 사용할 수 있다. FormatStyle 은 enum 값으로 FULL, LONG, MEDIUM, SHORT가 정의 되어있다. LocalDate day = LocalDate.of(2021, 9, 29); System.out.println(DateTimeFormatter.ofLocalizedDate(FormatStyle.FULL).format(day)); System.out.println(DateTimeFormatter.ofLocalizedDate(FormatStyle.LONG).format(day)); System.out.println(DateTimeFormatter.ofLocalizedDate(FormatStyle.MEDIUM).format(day)); System.out.println(DateTimeFormatter.ofLocalizedDate(FormatStyle.SHORT).format(day)); 출력은 다음과 같다. Wednesday, September 29, 2021 September 29, 2021 Sep 29, 2021 9/29/21 ZonedDateTime 인스턴스를 사용하여 날짜와 시간을 함께 표현할 수도 있다. LocalDate day = LocalDate.of(2021, 9, 29); LocalTime time = LocalTime.of(13, 12, 45); ZonedDateTime zonedDateTime = ZonedDateTime.of(day, time, ZoneId.of(\u0026#34;Asia/Seoul\u0026#34;)); System.out.println(DateTimeFormatter.ofLocalizedDateTime(FormatStyle.FULL).format(zonedDateTime)); System.out.println(DateTimeFormatter.ofLocalizedDateTime(FormatStyle.LONG).format(zonedDateTime)); System.out.println(DateTimeFormatter.ofLocalizedDateTime(FormatStyle.MEDIUM).format(zonedDateTime)); System.out.println(DateTimeFormatter.ofLocalizedDateTime(FormatStyle.SHORT).format(zonedDateTime)); 출력은 다음과 같다. Wednesday, September 29, 2021 at 1:12:45 PM Korean Standard Time September 29, 2021 at 1:12:45 PM KST Sep 29, 2021, 1:12:45 PM 9/29/21, 1:12 PM 반대로 문자열을 ZonedDateTime 으로 변경하고 싶다면 format() 메서드 대신 parse() 메서드를 사용한다. ZonedDateTime dateTime = ZonedDateTime.from(DateTimeFormatter.ofLocalizedDateTime(FormatStyle.FULL).parse(\u0026#34;Wednesday, September 29, 2021 at 1:12:45 PM Korean Standard Time\u0026#34;)); 사용자 정의 포맷 # 미리 정의된 포맷이 아닌 직접 포맷을 정의하여 사용하고 싶을 때가 있다. 이럴때에는 ofPattern() 메서드를 사용한다. String pattern = \u0026#34;yyyy-MM-dd\u0026#39;T\u0026#39;HH:mm:ss\u0026#34;; DateTimeFormatter formatter = DateTimeFormatter.ofPattern(pattern); System.out.println(formatter.format(LocalDateTime.now())); // 2021-09-29T12:26:18 패턴 문자의 수는 중요하다. 예를들어 month에 MM 과 같은 패턴을 사용한다면 1월을 \u0026ldquo;01\u0026quot;로 표기하며, M 과 같이 표기할 경우 1월을 \u0026ldquo;1\u0026quot;로 표기한다. 자주 사용하는 패턴 문자는 다음과 같다. 문자 의미 표시 예시 u year year 2004; 04 y year-of-era year 2004; 04 M/L month-of-year number/text 7; 07; Jul; July; J d day-of-month number 10 H hour-of-day (0-23) number 0 m minute-of-hour number 30 s second-of-minute number 55 S fraction-of-second number 978 n nano-of-second number 987654321 추가적인 패턴 문자를 알고 싶다면 Java documentation의 Pattern Letters and Symbols 표를 확인한다. 마무리 # 이번 포스트에서는 DateTimeFormatter 에 대하여 알아보았다. DateTimeFormatter 은 SimpleDateFormat 과 달리 스레드에 안전하고 더 최적화 되어있으므로 만약 java8 이상 버전을 사용한다면 SimpleDateFormat 대신 DateTimeFormatter 을 사용하자. 참고 자료: Guide to DateTimeFormatter","date":"2021-09-29T19:33:31+09:00","href":"https://disj11.github.io/date-time-formatter-in-java/","objectID":"70521835029d78afeba9b97ffa274d1a_0","order":0,"tags":["java"],"title":"Date Time Formatter in Java"},{"content":"개요 # Isolation level은 트랜잭션에서 일관성이 없는 데이터를 어느 수준까지 허용할 것인지 정의하는 데이터베이스의 중요한 개념입니다. 격리 수준이 낮을수록 여러 사용자가 동일한 데이터에 동시에 접근할 수 있어 성능이 향상되지만, 이로 인해 잘못된 데이터를 읽거나 데이터 업데이트가 손실되는 문제가 발생할 수 있습니다. 반대로 격리 수준이 높아질수록 데이터의 일관성은 보장되지만, 동시에 접근 가능한 사용자의 수가 줄어들어 성능이 저하될 수 있습니다. 이러한 균형을 맞추기 위해 대부분의 데이터베이스 시스템은 네 가지 격리 수준을 제공하며, 애플리케이션의 요구 사항에 따라 적절한 격리 수준을 선택할 수 있도록 설계되었습니다. 용어 설명 # 격리 수준을 이해하기 위해 먼저 Dirty Read, Non-repeatable Read, Phantom Read라는 개념을 알아야 합니다. 이는 각각 트랜잭션 간의 충돌로 인해 발생할 수 있는 대표적인 문제들입니다. Dirty Read # 커밋되지 않은 데이터를 다른 트랜잭션에서 읽는 것을 허용할 때 발생합니다. 이는 롤백된 데이터를 읽음으로써 잘못된 정보를 기반으로 동작하게 되는 문제를 초래합니다. Transaction 1 Transaction 2 SELECT age FROM users WHERE id = 1; UPDATE users SET age = 21 WHERE id = 1; SELECT age FROM users WHERE id = 1; ROLLBACK; 위 예시에서 Transaction 1은 커밋되지 않은 데이터를 읽었기 때문에, 최종적으로 롤백된 값(21)을 잘못 참조하게 됩니다. Non-repeatable Read # 한 트랜잭션 내에서 동일한 쿼리를 두 번 수행했을 때, 그 사이에 다른 트랜잭션이 데이터를 수정하거나 삭제하여 첫 번째 조회와 두 번째 조회 결과가 달라지는 현상입니다. Transaction 1 Transaction 2 SELECT age FROM users WHERE id = 1; UPDATE users SET age = 21 WHERE id = 1; COMMIT; SELECT age FROM users WHERE id = 1; COMMIT; 위 예시에서 Transaction 1은 같은 데이터를 두 번 조회했지만, 중간에 Transaction 2가 데이터를 수정했기 때문에 결과가 일관되지 않습니다. Phantom Read # 한 트랜잭션 내에서 일정 범위의 레코드를 두 번 이상 읽었을 때, 첫 번째 쿼리에서는 없었던 새로운 레코드가 이후 쿼리에서 나타나는 현상을 말합니다. 이는 주로 레코드 삽입과 관련된 문제입니다. Transaction 1 Transaction 2 SELECT age FROM users WHERE age \u0026lt; 20 INSERT INTO users(name, age) VALUES ('홍길동', 10); COMMIT; SELECT age FROM users WHERE age \u0026lt; 20; COMMIT; 위 예시에서 Transaction 1은 처음에는 조건에 맞는 레코드가 없었지만, 중간에 Transaction 2가 새로운 레코드를 삽입하면서 결과가 달라집니다. Isolation Level # 데이터베이스는 위와 같은 문제를 해결하기 위해 네 가지 격리 수준을 제공합니다. 각 격리 수준은 허용되는 동시성 문제와 성능 간의 균형을 다르게 설정합니다. Read Uncommitted # 커밋되지 않은 데이터를 다른 트랜잭션이 읽는 것을 허용합니다. 가장 낮은 격리 수준으로, Dirty Read, Non-repeatable Read, Phantom Read 문제가 모두 발생할 수 있습니다. 성능은 가장 뛰어나지만 데이터 일관성이 낮습니다. Read Committed # 커밋된 데이터만 다른 트랜잭션이 읽는 것을 허용합니다. Dirty Read 문제는 방지하지만, 여전히 Non-repeatable Read와 Phantom Read 문제가 발생할 수 있습니다. 대부분의 DBMS가 기본 격리 수준으로 채택하고 있는 모드입니다. Repeatable Read # 한 트랜잭션이 읽은 데이터를 다른 트랜잭션이 수정하거나 삭제하지 못하도록 보장합니다. Non-repeatable Read 문제를 방지하며, 동일한 데이터를 여러 번 조회해도 항상 일관된 결과를 얻을 수 있습니다. 그러나 여전히 Phantom Read 문제가 발생할 가능성이 있습니다. Serializable # 가장 높은 격리 수준으로, 모든 트랜잭션을 직렬화하여 처리하는 것처럼 동작합니다. 선행 트랜잭션이 읽은 데이터에 대해 후행 트랜잭션이 수정, 삭제뿐만 아니라 새로운 레코드 삽입도 불가능하게 만듭니다. 완벽한 데이터 일관성을 보장하지만 성능 저하가 크며, 동시성 처리 능력이 크게 제한됩니다. 결론 # Isolation level은 트랜잭션 간의 동시성과 데이터 일관성 사이에서 균형을 맞추기 위한 중요한 설정입니다. 애플리케이션의 요구 사항과 시스템 성능 목표에 따라 적절한 격리 수준을 선택하는 것이 중요합니다. 예를 들어, 금융 시스템처럼 높은 데이터 일관성이 요구되는 경우에는 Serializable 수준을 고려해볼 수 있으며, 반대로 높은 성능과 동시성이 중요한 경우에는 Read Committed나 Read Uncommitted를 선택할 수 있습니다.","date":"2021-09-28T20:30:02+09:00","href":"https://disj11.github.io/understanding-isolation-level-in-database-management/","objectID":"b01a54df9b1a54a03c056e00f3bb0052_0","order":0,"tags":["database"],"title":"트랜잭션 격리 수준(Isolation Level)의 이해: 데이터 일관성과 성능의 균형"},{"content":"개요 # 유니온 파인드는 Disjoint Set (서로소 집합) 또는 Merge-Find Set (병합-찾기 집합) 으로 불리며, 서로소인 집합들을 표현하는 자료구조입니다. 여기서 서로소 집합이란, 각 집합 간에 교집합의 원소가 하나도 없으며, 모든 집합의 합집합이 전체 원소를 포함하는 경우를 의미합니다. 유니온 파인드는 기본적으로 union(집합 병합)과 find(루트 찾기)라는 두 가지 연산만을 지원합니다. 예시 # 초기 상태에서는 다음과 같은 8개의 서로소 집합이 존재한다고 가정하겠습니다. {1}, {2}, {3}, {4}, {5}, {6}, {7}, {8} 위 상태는 초기의 크기가 8인 유니온 파인드 자료구조를 나타내며, 각각 독립적인 집합을 포함하고 있습니다. 몇 번의 연산이 이루어진 후, 다음과 같은 형태로 집합이 병합되었다고 가정해보겠습니다. {1, 2, 5, 6, 8}, {3, 4}, {7} 아래 그림은 병합 이후의 상태를 트리 구조로 표현한 것입니다. 트리에서 가장 위에 있는 노드를 루트(root) 라고 하며, 해당 트리에 속한 모든 정점은 동일한 집합에 속해 있습니다. 같은 집합을 표현하는 방법은 여러 가지가 있을 수 있으며, 위 그림은 그 중 하나의 예시입니다. 구현 # find 연산 # find 연산은 두 원소가 같은 집합에 속해 있는지 확인하기 위해 사용됩니다. 이를 위해 두 원소의 루트를 찾아서 루트가 동일한지 비교합니다. find 연산은 특정 원소의 루트를 찾아주는 역할을 합니다. 다음은 find 연산의 기본 구현입니다. /** * 원소 n을 받아 n의 root 노드를 반환합니다. * @param n 루트를 찾을 정점 * @return root 노드 */ int find(int n) { if (parent[n] \u0026lt; 0) { return n; // 자신이 루트 노드일 경우 } return find(parent[n]); // 재귀적으로 부모 노드를 따라 올라감 } 위 코드는 parent 배열을 사용하여 각 노드의 부모를 저장한다고 가정한 구현입니다. 여기서 루트 노드는 parent[n] \u0026lt; 0으로 표시됩니다. 하지만 이 방식에는 단점이 있습니다. 예를 들어, 위 그림과 같은 트리에서 find(6)을 호출하면, 경로가 일직선으로 길어질 경우 많은 재귀 호출이 발생하게 됩니다. 이를 개선하기 위해 경로 압축(path compression) 을 적용할 수 있습니다. 아래는 경로 압축을 적용한 개선된 코드입니다. int find(int n) { if (parent[n] \u0026lt; 0) { return n; } parent[n] = find(parent[n]); // 부모를 루트로 직접 설정하여 경로 압축 return parent[n]; } 경로 압축은 재귀 호출 과정에서 트리의 구조를 평평하게 만들어줍니다. 따라서 이후의 find 연산이 더 빠르게 수행될 수 있습니다. 경로 압축을 적용한 결과는 아래 그림과 같이 나타납니다. 경로 압축을 적용한 find 연산의 시간 복잡도는 매우 효율적이며, 이론적으로 \\( O(\\log^* N) \\)이라고 표현됩니다. 여기서 \\( \\log^* N \\)은 로그 스타 함수라고 불리며, 매우 느리게 증가하기 때문에 실질적으로 상수 시간에 가까운 성능을 보입니다. 로그 스타 함수 # 로그 스타 함수(\\( \\log^* N \\))는 반복 로그 함수라고도 불리며, 매우 느리게 증가하는 함수입니다. 이는 다음과 같이 정의됩니다: \\( \\log^* N \\): 어떤 양수 \\( N \\)에 대해 로그를 반복적으로 취해 그 결과가 \\( 1 \\) 이하가 될 때까지 몇 번 로그를 취했는지를 나타냅니다. 예를 들어: \\( N = 16 \\): \\( \\log_2(16) = 4 \\), \\( \\log_2(4) = 2 \\), \\( \\log_2(2) = 1 \\). 따라서 \\( \\log^*(16) = 3 \\). \\( N = 2^{65536} \\): 매우 큰 값임에도 불구하고 \\( \\log^*(N) = 5 \\). union 연산 # union 연산은 두 개의 서로소 집합을 하나로 합치는 작업을 수행합니다. 유니온 파인드 자료구조에서는 find 연산과 union 연산만 제공되며, 한 번 합쳐진 집합을 다시 분리하는 것은 어렵습니다. 그러나 보통 합치는 작업만 필요한 경우에 유용하게 사용됩니다. 다음은 union 연산의 기본 구현입니다. void merge(int a, int b) { a = find(a); // a의 루트를 찾음 b = find(b); // b의 루트를 찾음 if (a != b) { parent[b] = a; // b의 루트를 a로 변경하여 병합 } } 위 코드에서는 두 원소 \\( a \\)와 \\( b \\)가 속한 집합의 루트를 각각 찾은 후, \\( a \\)와 \\( b \\)가 서로 다른 집합에 속해 있을 경우 \\( b \\)의 루트를 \\( a \\)로 설정하여 병합합니다. 여기서 함수명을 merge로 사용한 이유는 C 언어에서 union이 예약어로 사용되기 때문입니다. 실제 구현에서는 union이라는 이름 대신 다른 이름을 사용하는 경우도 많습니다. 시간 복잡도 # find 연산: 경로 압축을 적용하면 이론적으로 \\( O(\\log^* N) \\), 실질적으로 상수 시간에 가까움. union 연산: find 연산에 의존하므로 동일하게 \\( O(\\log^* N) \\). 따라서 크기가 \\( N \\)인 유니온 파인드 자료구조에서 \\( M \\)번의 union 또는 find 연산을 수행할 경우 최악의 시간 복잡도는 \\( O(M \\cdot \\log^* N) \\)입니다. 참고자료: https://kks227.blog.me/","date":"2019-06-05T16:00:10+09:00","href":"https://disj11.github.io/union-find/","objectID":"c2d05c63b19742ca42a55a5458f49f6c_0","order":0,"tags":["data structure"],"title":"유니온 파인드"}]